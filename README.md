# Arabic Text Diacritization using Deep BiLSTM & Transformers
## üìå Project Overview

This project implements a robust deep learning pipeline for the automatic diacritization of Arabic text. The system utilizes a hybrid embedding approach combining state-of-the-art Transformer models with custom character-level features, fed into a Residual Bidirectional LSTM network. The goal is to restore the correct diacritical marks (Tashkeel) to raw Arabic text, preserving grammatical and semantic context.
## ‚öôÔ∏è Preprocessing Module

Data preparation is handled through a rigorous pipeline designed to maximize model stability and context retention:

    Text Cleaning:

        We utilize Regular Expressions (Regex) to sanitize the dataset. This involves removing non-Arabic characters, English letters, numbers, and special symbols to ensure the model focuses exclusively on Arabic phonology.

    Context-Aware Segmentation:

        To handle variable sequence lengths without truncating sentences arbitrarily, we calculated the distribution of sentence lengths across the training set.

        We established a maximum sequence length covering 99% of the training data.

        Instead of hard cutting at this limit, the splitter locates the nearest whitespace after the threshold. This ensures words are not broken in half, preserving the semantic context required for accurate diacritization.

    Citation Handling:

        Citations and religious text references often follow distinct grammatical rules compared to Modern Standard Arabic (MSA). These were split and processed separately to prevent their unique structures from confusing the general grammar model.

## üß† Feature Extraction (Embeddings)

We employed a hybrid embedding strategy to capture both semantic meaning and morphological character details:

    Word-Level Embeddings (AraElectra): We utilized AraElectra as our primary word embedding model. It provided superior contextual representation compared to other transformer models tested.

    Character-Level Embeddings: To capture the morphology of Arabic words (where diacritics are determined by character patterns), we implemented a custom character-level embedding layer in TensorFlow. This was achieved by training a preliminary network on raw characters and extracting the learned weights.

    Experiments & Ablations:

        AraBERT: Tested during the experimental phase but was replaced by AraElectra in the final pipeline.

        Flair Embeddings: We experimented with Flair contextual string embeddings; however, they did not yield a representative advantage for our specific character-level tasks and were discarded.

## üèóÔ∏è Model Architecture

The core of our diacritizer is a Deep Residual Bidirectional LSTM network designed to handle long-range dependencies in text:

    Input & Masking: The model begins with a masking layer to ignore padding values, ensuring the network processes only valid data.

    Projection Layer: Input features are projected via a Dense layer with ReLU activation and Dropout for regularization.

    Residual BiLSTM Blocks:

        The network features a stack of Bidirectional LSTM layers.

        Residual Connections (Skip Connections): Crucially, we employ residual connections (adding the input of a layer to its output) between LSTM blocks. This allows gradients to flow through the network more easily, enabling us to train a deeper model without suffering from the vanishing gradient problem.

        Dropout is applied after every block to prevent overfitting.

    Output Head:

        A final BiLSTM layer feeds into a Dense projection layer.

        The output is generated by a Softmax layer that predicts the probability distribution over the diacritic classes (including no diacritic).

## üöÄ Training Configuration

The training process was optimized for convergence and handling class imbalance:

    Loss Function: We used Sparse Categorical Focal Loss (gamma=2.0). This is critical for diacritization, as some diacritics (like Shadda combinations) are much rarer than others (like Fatha). Focal loss forces the model to focus on these hard-to-classify examples.

    Optimization: The Adam optimizer was used for weight updates.

    Learning Rate Scheduler: A ReduceLROnPlateau callback monitors validation loss. If improvement stalls, the learning rate is reduced by a factor of 0.5 (with a patience of 1 epoch) to fine-tune the weights.

    Custom Metrics: We track Masked Accuracy, which calculates accuracy strictly on valid tokens, ignoring padding indices to provide a realistic performance metric.

    Data Pipeline: The dataset uses TensorFlow's tf.data API with padded batching, infinite repeating, and AUTOTUNE prefetching to maximize GPU utilization.

## üìä Results

AraElectra (Final Model) Final Accuracy: AraBERT (Experimental) Final Accuracy:
## References
<p>https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9274427</p>
<img width="432" height="363" alt="image" src="https://github.com/user-attachments/assets/e7ee0999-378b-454e-b98a-d045ef08cf12" />
<p>https://aclanthology.org/2021.findings-emnlp.317.pdf</p>
<img width="779" height="336" alt="image" src="https://github.com/user-attachments/assets/ad274f3a-9685-4bc0-bbde-c16ed9ce2782" />
