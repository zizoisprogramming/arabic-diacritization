{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lIA95EyTmrHd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T14:46:13.430398Z",
          "iopub.status.busy": "2025-11-30T14:46:13.430193Z",
          "iopub.status.idle": "2025-11-30T14:46:18.242825Z",
          "shell.execute_reply": "2025-11-30T14:46:18.241977Z",
          "shell.execute_reply.started": "2025-11-30T14:46:13.430377Z"
        },
        "id": "lIA95EyTmrHd",
        "outputId": "176af2bb-24bc-4814-98d7-4df235b8ca74",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\n",
            "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
            "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q \"protobuf==3.20.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be8abeb-b020-4b07-a672-1bbf5e4fc006",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:46:18.245323Z",
          "iopub.status.busy": "2025-11-30T14:46:18.245053Z",
          "iopub.status.idle": "2025-11-30T14:46:32.661638Z",
          "shell.execute_reply": "2025-11-30T14:46:32.660978Z",
          "shell.execute_reply.started": "2025-11-30T14:46:18.245299Z"
        },
        "id": "1be8abeb-b020-4b07-a672-1bbf5e4fc006",
        "outputId": "531c0370-0e96-4bb6-976a-c80acf231445",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-30 14:46:19.364594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764513979.511954      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764513979.557742      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Bidirectional, Input\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f819122-f1ce-4a03-b147-c5b5274813d1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:46:32.662819Z",
          "iopub.status.busy": "2025-11-30T14:46:32.662354Z",
          "iopub.status.idle": "2025-11-30T14:46:32.667065Z",
          "shell.execute_reply": "2025-11-30T14:46:32.666343Z",
          "shell.execute_reply.started": "2025-11-30T14:46:32.6628Z"
        },
        "id": "2f819122-f1ce-4a03-b147-c5b5274813d1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "diacritic2id_path = \"/kaggle/input/arabicia-2/diacritic2id.json\"\n",
        "idx2char_path = \"/kaggle/input/arabicia-2/idx2char.json\"\n",
        "char2idx_path = \"/kaggle/input/arabicia-2/char2idx.json\"\n",
        "\n",
        "y_train_path = \"/kaggle/input/arabicia-2/cleaned_tashkeel.txt\"\n",
        "y_val_path = \"/kaggle/input/arabicia-2/cleaned_tashkeel_val.txt\"\n",
        "\n",
        "padded_train = \"/kaggle/input/padded/padded.pkl\"\n",
        "padded_val = \"/kaggle/input/padded/padded_val.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a519aca",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:46:32.668076Z",
          "iopub.status.busy": "2025-11-30T14:46:32.667836Z",
          "iopub.status.idle": "2025-11-30T14:46:32.936543Z",
          "shell.execute_reply": "2025-11-30T14:46:32.93599Z",
          "shell.execute_reply.started": "2025-11-30T14:46:32.668054Z"
        },
        "id": "4a519aca",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with open(diacritic2id_path, 'r', encoding='utf-8') as f:\n",
        "    diacritic2id = json.load(f)\n",
        "\n",
        "id2label = {v: k for k, v in diacritic2id.items()}\n",
        "\n",
        "with open(char2idx_path, 'r', encoding='utf-8') as f:\n",
        "    char2idx = json.load(f)\n",
        "\n",
        "with open(idx2char_path, 'r', encoding='utf-8') as f:\n",
        "    idx2char = json.load(f)\n",
        "\n",
        "idx2char['1'] = '?'\n",
        "value = char2idx.pop('\\uFFFD')\n",
        "char2idx['?'] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5669c8-f0bd-4be0-8d55-b4092404caac",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:46:32.937371Z",
          "iopub.status.busy": "2025-11-30T14:46:32.937193Z",
          "iopub.status.idle": "2025-11-30T14:46:32.941972Z",
          "shell.execute_reply": "2025-11-30T14:46:32.941228Z",
          "shell.execute_reply.started": "2025-11-30T14:46:32.937357Z"
        },
        "id": "5b5669c8-f0bd-4be0-8d55-b4092404caac",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def recover(sentence, tashkeel, testing=False):\n",
        "    result = \"\"\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        result += sentence[i] + tashkeel[i]\n",
        "        i += 1\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092bb84d-5fef-4755-8620-656afb5f36fe",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:46:32.943284Z",
          "iopub.status.busy": "2025-11-30T14:46:32.942931Z",
          "iopub.status.idle": "2025-11-30T14:46:32.952747Z",
          "shell.execute_reply": "2025-11-30T14:46:32.952049Z",
          "shell.execute_reply.started": "2025-11-30T14:46:32.943266Z"
        },
        "id": "092bb84d-5fef-4755-8620-656afb5f36fe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_data_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        X_raw, y_raw = pickle.load(f)\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for text_seq, label_seq in zip(X_raw, y_raw):\n",
        "        x_ids = [c for c in text_seq]\n",
        "        y_ids = [t for t in label_seq]\n",
        "\n",
        "        X.append(x_ids)\n",
        "        y.append(y_ids)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88998c6d-f1ba-4a29-95e2-58a66cf8e296",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:46:32.954764Z",
          "iopub.status.busy": "2025-11-30T14:46:32.954301Z",
          "iopub.status.idle": "2025-11-30T14:46:32.962729Z",
          "shell.execute_reply": "2025-11-30T14:46:32.962011Z",
          "shell.execute_reply.started": "2025-11-30T14:46:32.954746Z"
        },
        "id": "88998c6d-f1ba-4a29-95e2-58a66cf8e296",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def remove_pads(sentence, tashkeel):\n",
        "    chars = []\n",
        "    tashkeelat = []\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i] != '<PAD>':\n",
        "            chars.append(sentence[i])\n",
        "            tashkeelat.append(tashkeel[i])\n",
        "    sentence_text = \"\".join(chars)\n",
        "    text = sentence_text.replace('\\uFFFD', '?')\n",
        "    return text, tashkeelat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e0c99c1-2de3-4b7d-8120-a771122f1d52",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:46:32.963828Z",
          "iopub.status.busy": "2025-11-30T14:46:32.963479Z",
          "iopub.status.idle": "2025-11-30T14:47:04.808351Z",
          "shell.execute_reply": "2025-11-30T14:47:04.807567Z",
          "shell.execute_reply.started": "2025-11-30T14:46:32.963801Z"
        },
        "id": "5e0c99c1-2de3-4b7d-8120-a771122f1d52",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sentences, tashkeel_sequences = load_data_pickle(padded_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54480d8a-ef95-4d30-a826-8fcebeed5c16",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:47:04.80972Z",
          "iopub.status.busy": "2025-11-30T14:47:04.809214Z",
          "iopub.status.idle": "2025-11-30T14:47:30.316886Z",
          "shell.execute_reply": "2025-11-30T14:47:30.316088Z",
          "shell.execute_reply.started": "2025-11-30T14:47:04.809695Z"
        },
        "id": "54480d8a-ef95-4d30-a826-8fcebeed5c16",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "new_sentences = []\n",
        "new_tashkeel = []\n",
        "for i in range(len(sentences)):\n",
        "    text, tashkeel = remove_pads(sentences[i], tashkeel_sequences[i])\n",
        "    new_sentences.append(text)\n",
        "    new_tashkeel.append(tashkeel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65fb4fbf-9015-48b9-bb3f-be736ef177c9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:47:30.318847Z",
          "iopub.status.busy": "2025-11-30T14:47:30.31772Z",
          "iopub.status.idle": "2025-11-30T14:47:30.33598Z",
          "shell.execute_reply": "2025-11-30T14:47:30.335132Z",
          "shell.execute_reply.started": "2025-11-30T14:47:30.318819Z"
        },
        "id": "65fb4fbf-9015-48b9-bb3f-be736ef177c9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sentence_lengths = [len(sentence) for sentence in new_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aff964d-f7e0-469b-acfc-4f4cc76ea23f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:56:36.526805Z",
          "iopub.status.busy": "2025-11-30T14:56:36.526273Z",
          "iopub.status.idle": "2025-11-30T14:56:36.531161Z",
          "shell.execute_reply": "2025-11-30T14:56:36.530367Z",
          "shell.execute_reply.started": "2025-11-30T14:56:36.52678Z"
        },
        "id": "5aff964d-f7e0-469b-acfc-4f4cc76ea23f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "files_config = []\n",
        "for i in range(0, 64001, 16000):\n",
        "    files_config.append((f'/kaggle/input/arabic-diacrirization-features-{i + 16000}/zizo_part_{i + 16000}.h5', sentence_lengths[i:i + 16000]))\n",
        "files_config.append((f'/kaggle/input/arabic-diacrirization-features-80254/zizo_part_80254.h5', sentence_lengths[80000:80254]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56750051",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:49:47.235999Z",
          "iopub.status.busy": "2025-11-30T14:49:47.235295Z",
          "iopub.status.idle": "2025-11-30T14:49:47.240254Z",
          "shell.execute_reply": "2025-11-30T14:49:47.239706Z",
          "shell.execute_reply.started": "2025-11-30T14:49:47.235974Z"
        },
        "id": "56750051",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 807\n",
        "CHAR_EMBED_DIM = 256\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "VOCAB_SIZE = len(char2idx)\n",
        "NUM_CLASSES = len(diacritic2id)\n",
        "DIRTY = False\n",
        "DIACRITICS_PATTERN = re.compile(r'[\\u064B-\\u0652]')\n",
        "INPUT_DIM = 1024 # bert + mine\n",
        "HIDDEN_DIM = 512\n",
        "total_sentences = sum([len(lengths) for _, lengths in files_config])\n",
        "STEPS_PER_EPOCH = total_sentences // BATCH_SIZE\n",
        "PADDING_INPUT = -99999.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d4a670-0330-4368-96ad-3c36faab3012",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:49:49.698945Z",
          "iopub.status.busy": "2025-11-30T14:49:49.698691Z",
          "iopub.status.idle": "2025-11-30T14:49:49.70444Z",
          "shell.execute_reply": "2025-11-30T14:49:49.703822Z",
          "shell.execute_reply.started": "2025-11-30T14:49:49.698927Z"
        },
        "id": "f4d4a670-0330-4368-96ad-3c36faab3012",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class SparseCategoricalFocalLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma=2.0, ignore_id=15, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.gamma = gamma\n",
        "        self.ignore_id = ignore_id\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.squeeze(y_true, axis=-1)\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "\n",
        "        mask = tf.cast(tf.not_equal(y_true, self.ignore_id), tf.float32)\n",
        "\n",
        "        y_true_safe = tf.where(y_true == self.ignore_id, 0, y_true)\n",
        "\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_safe, y_pred, from_logits=False)\n",
        "\n",
        "        p_t = tf.math.exp(-loss)\n",
        "        focal_loss = tf.math.pow(1.0 - p_t, self.gamma) * loss\n",
        "\n",
        "        masked_loss = focal_loss * mask\n",
        "\n",
        "        return tf.reduce_sum(masked_loss) / (tf.reduce_sum(mask) + 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89523874-72ec-42cc-b038-6693b2270e1a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:58:29.891921Z",
          "iopub.status.busy": "2025-11-30T14:58:29.891625Z",
          "iopub.status.idle": "2025-11-30T14:58:29.926653Z",
          "shell.execute_reply": "2025-11-30T14:58:29.926076Z",
          "shell.execute_reply.started": "2025-11-30T14:58:29.891903Z"
        },
        "id": "89523874-72ec-42cc-b038-6693b2270e1a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "def multi_file_generator():\n",
        "    for h5_path, lengths in files_config:\n",
        "\n",
        "        with h5py.File(h5_path, 'r') as f:\n",
        "            features_dset = f['features']\n",
        "            labels_dset = f['tashkeel']\n",
        "\n",
        "            current_idx = 0\n",
        "\n",
        "            for seq_len in lengths:\n",
        "                end_idx = current_idx + seq_len\n",
        "\n",
        "                feat = features_dset[current_idx : end_idx]\n",
        "                lbl = labels_dset[current_idx : end_idx]\n",
        "\n",
        "                if feat.shape[0] > 0:\n",
        "                    lbl = lbl.reshape(-1, 1)\n",
        "                    yield feat, lbl\n",
        "\n",
        "                current_idx = end_idx\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    multi_file_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, INPUT_DIM), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 1), dtype=tf.int32)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ddfe8e-7866-4b37-8d82-902b967e06b1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:58:30.716358Z",
          "iopub.status.busy": "2025-11-30T14:58:30.71612Z",
          "iopub.status.idle": "2025-11-30T14:58:30.727057Z",
          "shell.execute_reply": "2025-11-30T14:58:30.72642Z",
          "shell.execute_reply.started": "2025-11-30T14:58:30.716342Z"
        },
        "id": "09ddfe8e-7866-4b37-8d82-902b967e06b1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset = dataset.padded_batch(\n",
        "    BATCH_SIZE,\n",
        "    padding_values=(PADDING_INPUT, 15)\n",
        ")\n",
        "dataset = dataset.repeat()\n",
        "\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(None, INPUT_DIM))\n",
        "\n",
        "    masked = tf.keras.layers.Masking(mask_value=PADDING_INPUT)(inputs)\n",
        "\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HIDDEN_DIM, return_sequences=True))(masked)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HIDDEN_DIM, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    # x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HIDDEN_DIM, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HIDDEN_DIM, return_sequences=True))(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(HIDDEN_DIM, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cf727b-5347-45ac-8609-37489c84452d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:59:01.768369Z",
          "iopub.status.busy": "2025-11-30T14:59:01.767652Z",
          "iopub.status.idle": "2025-11-30T14:59:01.772654Z",
          "shell.execute_reply": "2025-11-30T14:59:01.772064Z",
          "shell.execute_reply.started": "2025-11-30T14:59:01.768344Z"
        },
        "id": "93cf727b-5347-45ac-8609-37489c84452d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def masked_accuracy(y_true, y_pred):\n",
        "    y_true = tf.squeeze(y_true, axis=-1)\n",
        "    y_true = tf.cast(y_true, tf.int32)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, 15), tf.float32)\n",
        "\n",
        "    y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n",
        "\n",
        "    correct = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)\n",
        "\n",
        "    masked_correct = correct * mask\n",
        "\n",
        "    return tf.reduce_sum(masked_correct) / (tf.reduce_sum(mask) + 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74979fba-033a-456a-a7f0-3c52f2b4fbcc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T14:59:02.406158Z",
          "iopub.status.busy": "2025-11-30T14:59:02.405566Z",
          "iopub.status.idle": "2025-11-30T16:16:31.008746Z",
          "shell.execute_reply": "2025-11-30T16:16:31.008047Z",
          "shell.execute_reply.started": "2025-11-30T14:59:02.406138Z"
        },
        "id": "74979fba-033a-456a-a7f0-3c52f2b4fbcc",
        "outputId": "6cbd48b9-e669-4a43-8f39-662ebeb49ed2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1764514749.233684     138 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m586s\u001b[0m 288ms/step - loss: 0.2888 - masked_accuracy: 0.8380\n",
            "Epoch 2/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step - loss: 0.0593 - masked_accuracy: 0.4741    \n",
            "Epoch 3/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m578s\u001b[0m 288ms/step - loss: 0.0410 - masked_accuracy: 0.9652\n",
            "Epoch 4/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5us/step - loss: 0.0391 - masked_accuracy: 0.4827    \n",
            "Epoch 5/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 288ms/step - loss: 0.0278 - masked_accuracy: 0.9747\n",
            "Epoch 6/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5us/step - loss: 0.0276 - masked_accuracy: 0.4880    \n",
            "Epoch 7/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 289ms/step - loss: 0.0210 - masked_accuracy: 0.9794\n",
            "Epoch 8/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5us/step - loss: 0.0224 - masked_accuracy: 0.4901    \n",
            "Epoch 9/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 289ms/step - loss: 0.0167 - masked_accuracy: 0.9825\n",
            "Epoch 10/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5us/step - loss: 0.0158 - masked_accuracy: 0.4913    \n",
            "Epoch 11/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 289ms/step - loss: 0.0138 - masked_accuracy: 0.9848\n",
            "Epoch 12/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step - loss: 0.0159 - masked_accuracy: 0.4915    \n",
            "Epoch 13/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 289ms/step - loss: 0.0117 - masked_accuracy: 0.9866\n",
            "Epoch 14/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5us/step - loss: 0.0082 - masked_accuracy: 0.4946    \n",
            "Epoch 15/15\n",
            "\u001b[1m2007/2007\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 289ms/step - loss: 0.0102 - masked_accuracy: 0.9879\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f068d61b790>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = build_model()\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=SparseCategoricalFocalLoss(gamma=2.0), # ,ignore_id=14),\n",
        "    metrics=[masked_accuracy]\n",
        ")\n",
        "\n",
        "model.fit(dataset,\n",
        "          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "          epochs=10,\n",
        "          verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84d4c61-7cb6-41d5-9b24-b7e43fc32d4f",
      "metadata": {
        "id": "a84d4c61-7cb6-41d5-9b24-b7e43fc32d4f"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e1a6bf-e926-4e7f-b105-2f5fb5150d29",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-28T16:48:21.805574Z",
          "iopub.status.idle": "2025-11-28T16:48:21.805797Z",
          "shell.execute_reply": "2025-11-28T16:48:21.805702Z",
          "shell.execute_reply.started": "2025-11-28T16:48:21.805692Z"
        },
        "id": "02e1a6bf-e926-4e7f-b105-2f5fb5150d29",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def another_smart_slicer_generator(lengths, offset=0):\n",
        "    with h5py.File('/kaggle/input/arabic-diacrirization-features-val-3907/zizo_part_3907.h5', 'r') as f:\n",
        "        features_dset = f['features']\n",
        "        labels_dset = f['tashkeel']\n",
        "\n",
        "        current_idx = offset\n",
        "\n",
        "        for seq_len in lengths:\n",
        "            end_idx = current_idx + seq_len\n",
        "\n",
        "            feat = features_dset[current_idx : end_idx]\n",
        "            lbl = labels_dset[current_idx : end_idx]\n",
        "\n",
        "            lbl = lbl.reshape(-1, 1)\n",
        "\n",
        "            yield feat, lbl\n",
        "\n",
        "            current_idx = end_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5968357e-83ae-48a2-9cfe-4aed885a8d02",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-28T16:48:21.806992Z",
          "iopub.status.idle": "2025-11-28T16:48:21.807247Z",
          "shell.execute_reply": "2025-11-28T16:48:21.807153Z",
          "shell.execute_reply.started": "2025-11-28T16:48:21.807143Z"
        },
        "id": "5968357e-83ae-48a2-9cfe-4aed885a8d02",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sentences, tashkeel_sequences = load_data_pickle(padded_val)\n",
        "new_sentences = []\n",
        "new_tashkeel = []\n",
        "for i in range(len(sentences)):\n",
        "    text, tashkeel = remove_pads(sentences[i], tashkeel_sequences[i])\n",
        "    new_sentences.append(text)\n",
        "    new_tashkeel.append(tashkeel)\n",
        "sentence_lengths = [len(sentence) for sentence in new_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeefb3f7-36a1-4442-a221-e9c2577b897b",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-28T16:48:21.808637Z",
          "iopub.status.idle": "2025-11-28T16:48:21.808892Z",
          "shell.execute_reply": "2025-11-28T16:48:21.80878Z",
          "shell.execute_reply.started": "2025-11-28T16:48:21.808769Z"
        },
        "id": "aeefb3f7-36a1-4442-a221-e9c2577b897b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: another_smart_slicer_generator(all_sentence_lengths, offset=0),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, INPUT_DIM), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 1), dtype=tf.int32)\n",
        "    )\n",
        ").padded_batch(BATCH_SIZE, padding_values=(PADDING_INPUT, 15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d002009-f860-4054-aee9-9ad383065845",
      "metadata": {
        "id": "9d002009-f860-4054-aee9-9ad383065845",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "total_correct = 0\n",
        "total_chars = 0\n",
        "sentences_processed = 0\n",
        "\n",
        "for batch_x, batch_y in test_dataset:\n",
        "\n",
        "    batch_probs = model.predict_on_batch(batch_x)\n",
        "\n",
        "    batch_pred_ids = np.argmax(batch_probs, axis=-1)\n",
        "\n",
        "    batch_true_ids = batch_y.numpy().squeeze(axis=-1)\n",
        "\n",
        "    batch_size = batch_pred_ids.shape[0]\n",
        "\n",
        "    for k in range(batch_size):\n",
        "\n",
        "        valid_indices = np.where(batch_true_ids[k] != 15)[0]\n",
        "\n",
        "        if len(valid_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        real_len = valid_indices[-1] + 1\n",
        "\n",
        "        p_seq = batch_pred_ids[k][:real_len]\n",
        "        t_seq = batch_true_ids[k][:real_len]\n",
        "\n",
        "        matches = np.sum(p_seq == t_seq)\n",
        "\n",
        "        total_correct += matches\n",
        "        total_chars += real_len\n",
        "        sentences_processed += 1\n",
        "\n",
        "if total_chars > 0:\n",
        "    print(f\"Final Accuracy: {total_correct / total_chars:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc26f67b-1eff-4189-801c-edfb47f7cf23",
      "metadata": {
        "id": "fc26f67b-1eff-4189-801c-edfb47f7cf23"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8a5020-0454-4973-be14-13807d265a84",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T16:16:31.010065Z",
          "iopub.status.busy": "2025-11-30T16:16:31.009857Z",
          "iopub.status.idle": "2025-11-30T16:16:31.517667Z",
          "shell.execute_reply": "2025-11-30T16:16:31.517046Z",
          "shell.execute_reply.started": "2025-11-30T16:16:31.010051Z"
        },
        "id": "7f8a5020-0454-4973-be14-13807d265a84",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Standard Keras format (Recommended)\n",
        "model.save('model_with_features_v1.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fb3170-4371-4faf-a95c-5452354825ff",
      "metadata": {
        "id": "82fb3170-4371-4faf-a95c-5452354825ff",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_test_inference(test_sentences, bert_model, bert_tokenizer, classifier_model, batch_size=32):\n",
        "\n",
        "    all_features = []\n",
        "    all_lengths = []\n",
        "\n",
        "    print(\"Extracting features...\")\n",
        "    for i, sent in tqdm(enumerate(test_sentences), total=len(test_sentences)):\n",
        "        feat, _ = zizo_features(sent, i, bert_model, bert_tokenizer)\n",
        "\n",
        "        all_features.append(feat)\n",
        "        all_lengths.append(len(feat))\n",
        "\n",
        "    def generator():\n",
        "        for feat in all_features:\n",
        "            yield feat\n",
        "\n",
        "    feature_dim = all_features[0].shape[1]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=tf.TensorSpec(shape=(None, feature_dim), dtype=tf.float32)\n",
        "    )\n",
        "\n",
        "    dataset = dataset.padded_batch(batch_size, padding_values=0.0)\n",
        "\n",
        "    print(\"Running inference...\")\n",
        "    predictions = classifier_model.predict(dataset, verbose=1)\n",
        "\n",
        "    predicted_ids = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    final_diacritized_sentences = []\n",
        "\n",
        "    print(\"Reconstructing text...\")\n",
        "    for i, length in enumerate(all_lengths):\n",
        "        sentence_pred_ids = predicted_ids[i][:length]\n",
        "\n",
        "        original_text = test_sentences[i]\n",
        "\n",
        "        res = \"\"\n",
        "        for char_idx, char in enumerate(original_text):\n",
        "            if char_idx < len(sentence_pred_ids):\n",
        "                d_id = sentence_pred_ids[char_idx]\n",
        "                diacritic = id2diacritic[d_id] if d_id in id2diacritic else \"\"\n",
        "                res += char + diacritic\n",
        "            else:\n",
        "                res += char\n",
        "\n",
        "        final_diacritized_sentences.append(res)\n",
        "\n",
        "    return final_diacritized_sentences"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "training",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8861767,
          "sourceId": 13908408,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8862456,
          "sourceId": 13909321,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8878886,
          "sourceId": 13932666,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8878927,
          "sourceId": 13932714,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8878961,
          "sourceId": 13932757,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8879007,
          "sourceId": 13932818,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8879046,
          "sourceId": 13932865,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
