{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbb5f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77dc6f",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73ace33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pattern = r\"\\(\\s*\\d+\\s*/\\s*\\d+\\s*\\)\"\n",
    "english = r\"[a-zA-Z]\"\n",
    "numbers = r\"\\s*\\d+\\s*\"\n",
    "numering_items = r\"\\s*\\d+\\s*[-]\\s*\"\n",
    "empty_brackets = r'\\(\\s*\\)|\\[\\s*\\]|\\{\\s*\\}|<<\\s*>>|\"\\s*\"|\\'\\s*\\''\n",
    "stand_alone=r'(?<=\\s|\\^|\\(|\\[|\\{)[^\\(\\)\\[\\]\\{\\}\\.,،:;؛؟!\\-](?=\\s|$|\\]|\\)|\\})'\n",
    "UNK_CHAR = '\\uFFFD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6eb25469",
   "metadata": {},
   "outputs": [],
   "source": [
    "kawloho_pattern = r\"(\\s*قَوْلُهُ\\s*)\"\n",
    "qala_variations = r\"(?:قَالَ|قَالَتْ|قُلْت|قَالُوا|قُلْنَا|أَقُولُ)\"\n",
    "qala_variations = r\"(?:قَالَ|قَالَتْ|قُلْت|قَالُوا|قُلْنَا|أَقُولُ)\"\n",
    "qala_pattern = rf\"(\\s*{qala_variations}\\s*:)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18c479ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unbalanced_brackets(text):\n",
    "    pair_map = {')': '(', '}': '{', ']': '[', '>':'<', '»': '«', '\"':'\"', \"'\":\"'\"}\n",
    "    openers = set(['(', '{', '[', '<', '«', '\"', \"'\"])\n",
    "    \n",
    "    stack = [] \n",
    "    indices_to_remove = set()\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char in openers:\n",
    "            stack.append((char, i))\n",
    "        \n",
    "        elif char in pair_map:\n",
    "            if stack:\n",
    "                last_opener, _ = stack[-1]\n",
    "                if last_opener == pair_map[char]:\n",
    "                    stack.pop()\n",
    "                else:\n",
    "                    indices_to_remove.add(i)\n",
    "            else:\n",
    "                indices_to_remove.add(i)\n",
    "\n",
    "    for char, index in stack:\n",
    "        indices_to_remove.add(index)\n",
    "\n",
    "    return \"\".join([char for i, char in enumerate(text) if i not in indices_to_remove])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff18ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation_sequence(text):\n",
    "    puncs = re.escape(\".,:;{}[]()!?'\\\"/،؛؟\")\n",
    "    pattern = rf\"([{puncs}])(?:\\s*[{puncs}])+\"\n",
    "    return re.sub(pattern, r\"\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "028c4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_citations(citation_pattern, lines):\n",
    "    final_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        modified_line = re.sub(citation_pattern, r\"\\n\\1\", line)\n",
    "        \n",
    "        parts = modified_line.split('\\n')\n",
    "        \n",
    "        for part in parts:\n",
    "            cleaned_part = part.strip()\n",
    "            if cleaned_part:\n",
    "                final_lines.append(cleaned_part)\n",
    "                \n",
    "    return final_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecd7e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation_sequence(text):\n",
    "    collapsible = re.escape(\".,:;!?'\\\"/،؛؟\")    \n",
    "    pattern = rf\"([{collapsible}])(?:\\s*\\1)+\"\n",
    "    \n",
    "    return re.sub(pattern, r\"\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8b1517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_citations(lines):\n",
    "    qal_list = [\n",
    "        \"قَالَ\",\"قَالَتْ\",\"قَالُوا\",\"قُلْت\",\"قُلْنَا\",\n",
    "        \"أَقُولُ\",\"يَقُولُ\",\"يَقُولُونَ\",\"قِيلَ\",\"يُقَالُ\"\n",
    "    ]\n",
    "    \n",
    "    def add_tashkeel(word):\n",
    "        tashkeel = \"[\\u064B-\\u065F]*\"\n",
    "        return \"\".join([c + tashkeel for c in word])\n",
    "    \n",
    "    qal_regex = \"|\".join([add_tashkeel(w) for w in qal_list])\n",
    "\n",
    "    qal_with_colon = rf\"(?:{qal_regex})\\s*[:：]\"\n",
    "\n",
    "    tashkeel = \"[\\u064B-\\u065F]*\"\n",
    "    qawloho_regex = rf\"(?:وَ|فَ)?قَوْل{tashkeel}(?:ه{tashkeel}|هُ{tashkeel})?(?:\\s*تَعَالَى)?\"\n",
    "\n",
    "    trigger = rf\"({qal_with_colon}|{qawloho_regex})\"\n",
    "\n",
    "    final_lines = []\n",
    "    for line in lines:\n",
    "        matches = list(re.finditer(trigger, line))\n",
    "        if not matches:\n",
    "            final_lines.append(line.strip())\n",
    "            continue\n",
    "        \n",
    "        last_idx = 0\n",
    "        for m in matches:\n",
    "            start = m.start()\n",
    "            if line[last_idx:start].strip():\n",
    "                final_lines.append(line[last_idx:start].strip())\n",
    "            last_idx = start\n",
    "        \n",
    "        final_lines.append(line[last_idx:].strip())\n",
    "        \n",
    "    return final_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "adfd4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(lines):\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        res = re.sub(numering_items, '', line)\n",
    "        res = re.sub(numeric_pattern, '', res)\n",
    "        res = re.sub(english, ' ', res)\n",
    "        res = re.sub(numbers, ' ', res)\n",
    "        res = re.sub(empty_brackets, '', res)\n",
    "        res = re.sub(',', '،', res)\n",
    "        res = re.sub(';', '؛', res)\n",
    "        res = re.sub(r'\\s+ا\\s*هـ?\\s+', ' ، ', res)\n",
    "        res = re.sub(fr'\\((\\s*{stand_alone}\\s*)+\\)', f' {UNK_CHAR} ', res)\n",
    "        res = re.sub(fr'(\\s*{stand_alone}\\s*)+', f' {UNK_CHAR} ', res) \n",
    "        res = re.sub(r'/', '', res)\n",
    "        res = re.sub(r'\\*', '', res)\n",
    "        res = re.sub(r'–', '-', res)\n",
    "        res = res.replace('\\u200f', '')\n",
    "        \n",
    "        res = clean_punctuation_sequence(res)\n",
    "        res = remove_unbalanced_brackets(res)\n",
    "        \n",
    "        res = re.sub(r\"\\s+\", \" \", res).strip()\n",
    "        new_lines.append(res)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5f4bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path='../data/train.txt'):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return process_text(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5cb3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = read_data('../data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b05fd503",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines = split_citations(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3b2066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_lines = [remove_unbalanced_brackets(line) for line in new_lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d6a92",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af894c6",
   "metadata": {},
   "source": [
    "## TA code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be4bee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../utils/arabic_letters.pickle' \n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    arabic_letters = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b25ee56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../utils/diacritic2id.pickle' \n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    diacritic2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0dea69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [' ', '،', ':', '؛', '!', '؟', '\"', \"'\", '«', '»', '(', ')', '[', ']', '{', '}', '-', '.']\n",
    "DIACRITICS_PATTERN = re.compile(r'[\\u064B-\\u0652]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a9dbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(text, labels, overlap=50, max_len=200):\n",
    "    assert len(text) == len(labels), \"Text and labels must be of the same length.\"\n",
    "    if len(text) <= max_len:\n",
    "        return [text], [labels]\n",
    "    \n",
    "    text_chunks = []\n",
    "    label_chunks = []\n",
    "    \n",
    "    stride = max_len - overlap\n",
    "    \n",
    "    for i in range(0, len(text), stride):\n",
    "        t_chunk = text[i : i + max_len]\n",
    "        \n",
    "        l_chunk = labels[i : i + max_len]\n",
    "        \n",
    "        text_chunks.append(t_chunk)\n",
    "        label_chunks.append(l_chunk)\n",
    "        \n",
    "        if i + max_len >= len(text): \n",
    "            break\n",
    "            \n",
    "    return (text_chunks, label_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4841075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_and_diacritics(text):\n",
    "\n",
    "    letters = []\n",
    "    labels = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        char = text[i]\n",
    "        \n",
    "        if DIACRITICS_PATTERN.match(char):\n",
    "            if labels:\n",
    "                labels[-1] += char\n",
    "        \n",
    "        else:\n",
    "            letters.append(char)\n",
    "            labels.append(\"\") \n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    return \"\".join(letters), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01798c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "cleaned_tashkeel = []\n",
    "\n",
    "for line in cleaned_lines:\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    text, tashkeel = split_text_and_diacritics(line)\n",
    "    if len(text) <= 5:\n",
    "        continue\n",
    "    \n",
    "    cleaned_text.append(text)\n",
    "    cleaned_tashkeel.append(tashkeel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1480b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cleaned_text)):\n",
    "    assert len(cleaned_text[i]) == len(cleaned_tashkeel[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed3c5b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile = 0.99\n",
    "lengths = [len(text) for text in cleaned_text]\n",
    "MAX_LEN = int(sorted(lengths)[int(len(lengths) * quantile)])\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0841fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_lines = []\n",
    "chunked_labels = []\n",
    "for i, new_line in enumerate(cleaned_text):\n",
    "    chunks = slide_window(new_line, cleaned_tashkeel[i], overlap=50, max_len=MAX_LEN)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks[0]):\n",
    "        chunked_lines.append(chunk)\n",
    "        chunked_labels.append(chunks[1][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aefebf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(chunked_lines)):\n",
    "    assert len(chunked_lines[i]) == len(chunked_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06e2f7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char '�' not found in arabic_letters list.\n"
     ]
    }
   ],
   "source": [
    "char_set = {}\n",
    "for line in chunked_lines:\n",
    "\n",
    "    for char in line:\n",
    "        if char in char_set:\n",
    "            continue\n",
    "        else:\n",
    "            if char not in arabic_letters and char not in punctuation and char not in diacritic2id.keys():\n",
    "                print(f\"Char {repr(char)} not found in arabic_letters list.\")   \n",
    "            char_set[char] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3dd4d0",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33921a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(tokens_text, tokens_labels, max_len):\n",
    "    assert len(tokens_text) == len(tokens_labels), \"Tokens and labels must be of the same length.\"\n",
    "    if len(tokens_text) == max_len:\n",
    "        return (tokens_text[:max_len], tokens_labels[:max_len])\n",
    "    elif len(tokens_text) < max_len:\n",
    "        return (tokens_text + ['<PAD>'] * (max_len - len(tokens_text)), \n",
    "                tokens_labels + [''] * (max_len - len(tokens_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f60de732",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m padded_text, padded_label = pad_tokens(listed_line, chunked_labels[i], MAX_LEN)\n\u001b[32m     10\u001b[39m padded_texts.append(padded_text)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m padded_labels.append(padded_label)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "padded_texts = []\n",
    "padded_labels = []\n",
    "\n",
    "for i, line in enumerate(chunked_lines):\n",
    "    listed_line = []\n",
    "    for char in line:\n",
    "        listed_line.append(char) \n",
    "            \n",
    "    padded_text, padded_label = pad_tokens(listed_line, chunked_labels[i], MAX_LEN)\n",
    "    padded_texts.append(padded_text)\n",
    "    padded_labels.append(padded_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(padded_texts)):\n",
    "    assert len(padded_texts[i]) == len(padded_labels[i]) == MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e8bfe",
   "metadata": {},
   "source": [
    "## Vocab building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ea0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(padded_texts, og_lines):\n",
    "    \n",
    "    word_set = {}\n",
    "    # i = 0\n",
    "    # for line in og_lines:\n",
    "    #     for word in line.split():\n",
    "    #         if word == UNK_CHAR:\n",
    "    #             continue\n",
    "    #         if word not in word_set:\n",
    "    #             word_set[word] = i\n",
    "    #             i += 1                \n",
    "\n",
    "    char2idx = {\n",
    "        '<PAD>' : 0,\n",
    "        UNK_CHAR : 1\n",
    "    }\n",
    "    i = 2\n",
    "    for text in padded_texts:\n",
    "        for char in text:\n",
    "            if char not in char2idx:\n",
    "                char2idx[char] = i\n",
    "                i += 1\n",
    "    \n",
    "    return char2idx, word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx, word_vocab = build_vocab(padded_texts, chunked_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ebde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = {v: k for k, v in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in arabic_letters:\n",
    "    if letter not in char2idx:\n",
    "        assert False, f\"Letter {repr(letter)} from arabic_letters not found in vocab.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in char2idx:\n",
    "    if letter not in arabic_letters and letter not in punctuation and letter != '<PAD>' and letter != UNK_CHAR:\n",
    "        print(f\"Letter {repr(letter)} from vocab not found in arabic_letters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb20002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in chunked_lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "with open('../data/cleaned_tashkeel.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in chunked_labels:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949150b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/padded_dirty.pkl', 'wb') as f:\n",
    "    pickle.dump((padded_texts, padded_labels), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/char2idx.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(char2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('../data/idx2char.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2char, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned_val.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in chunked_lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "with open('../data/cleaned_tashkeel_val.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in chunked_labels:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd929948",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {v: k for k, v in diacritic2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned_all.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(padded_texts)):\n",
    "        result_str = \"\"\n",
    "        for char, p_id in zip(padded_texts[i], padded_labels[i]):\n",
    "            if char == '<PAD>': \n",
    "                break \n",
    "            \n",
    "            result_str += char + p_id\n",
    "        f.write(result_str + \"\\n\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
