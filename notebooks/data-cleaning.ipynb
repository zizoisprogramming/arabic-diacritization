{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cbb5f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5ad9af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77dc6f",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "73ace33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pattern = r\"\\(\\s*\\d+\\s*/\\s*\\d+\\s*\\)\"\n",
    "english = r\"[a-zA-Z]\"\n",
    "numbers = r\"\\s*\\d+\\s*\"\n",
    "numering_items = r\"\\s*\\d+\\s*[-]\\s*\"\n",
    "empty_brackets = r'\\(\\s*\\)|\\[\\s*\\]|\\{\\s*\\}|<<\\s*>>|\"\\s*\"|\\'\\s*\\''\n",
    "stand_alone=r'(?<=\\s|\\^|\\(|\\[|\\{)[^\\(\\)\\[\\]\\{\\}\\.,،:;؛؟!\\-](?=\\s|$|\\]|\\)|\\})'\n",
    "UNK_CHAR = '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6eb25469",
   "metadata": {},
   "outputs": [],
   "source": [
    "kawloho_pattern = r\"(\\s*قَوْلُهُ\\s*)\"\n",
    "qala_variations = r\"(?:قَالَ|قَالَتْ|قُلْت|قَالُوا|قُلْنَا|أَقُولُ)\"\n",
    "qala_variations = r\"(?:قَالَ|قَالَتْ|قُلْت|قَالُوا|قُلْنَا|أَقُولُ)\"\n",
    "qala_pattern = rf\"(\\s*{qala_variations}\\s*:)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "18c479ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unbalanced_brackets(text):\n",
    "    pair_map = {')': '(', '}': '{', ']': '[', '>':'<', '»': '«', '\"':'\"', \"'\":\"'\"}\n",
    "    openers = set(['(', '{', '[', '<', '«', '\"', \"'\"])\n",
    "    \n",
    "    stack = [] \n",
    "    indices_to_remove = set()\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char in openers:\n",
    "            stack.append((char, i))\n",
    "        \n",
    "        elif char in pair_map:\n",
    "            if stack:\n",
    "                last_opener, _ = stack[-1]\n",
    "                if last_opener == pair_map[char]:\n",
    "                    stack.pop()\n",
    "                else:\n",
    "                    indices_to_remove.add(i)\n",
    "            else:\n",
    "                indices_to_remove.add(i)\n",
    "\n",
    "    for char, index in stack:\n",
    "        indices_to_remove.add(index)\n",
    "\n",
    "    return \"\".join([char for i, char in enumerate(text) if i not in indices_to_remove])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ff18ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation_sequence(text):\n",
    "    puncs = re.escape(\".,:;{}[]()!?'\\\"/،؛؟\")\n",
    "    pattern = rf\"([{puncs}])(?:\\s*[{puncs}])+\"\n",
    "    return re.sub(pattern, r\"\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "028c4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_citations(citation_pattern, lines):\n",
    "    final_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        modified_line = re.sub(citation_pattern, r\"\\n\\1\", line)\n",
    "        \n",
    "        parts = modified_line.split('\\n')\n",
    "        \n",
    "        for part in parts:\n",
    "            cleaned_part = part.strip()\n",
    "            if cleaned_part:\n",
    "                final_lines.append(cleaned_part)\n",
    "                \n",
    "    return final_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ecd7e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation_sequence(text):\n",
    "    collapsible = re.escape(\".,:;!?'\\\"/،؛؟\")    \n",
    "    pattern = rf\"([{collapsible}])(?:\\s*\\1)+\"\n",
    "    \n",
    "    return re.sub(pattern, r\"\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e8b1517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_citations(lines):\n",
    "    qal_list = [\n",
    "        \"قَالَ\",\"قَالَتْ\",\"قَالُوا\",\"قُلْت\",\"قُلْنَا\",\n",
    "        \"أَقُولُ\",\"يَقُولُ\",\"يَقُولُونَ\",\"قِيلَ\",\"يُقَالُ\"\n",
    "    ]\n",
    "    \n",
    "    def add_tashkeel(word):\n",
    "        tashkeel = \"[\\u064B-\\u065F]*\"\n",
    "        return \"\".join([c + tashkeel for c in word])\n",
    "    \n",
    "    qal_regex = \"|\".join([add_tashkeel(w) for w in qal_list])\n",
    "\n",
    "    qal_with_colon = rf\"(?:{qal_regex})\\s*[:：]\"\n",
    "\n",
    "    tashkeel = \"[\\u064B-\\u065F]*\"\n",
    "    qawloho_regex = rf\"(?:وَ|فَ)?قَوْل{tashkeel}(?:ه{tashkeel}|هُ{tashkeel})?(?:\\s*تَعَالَى)?\"\n",
    "\n",
    "    trigger = rf\"({qal_with_colon}|{qawloho_regex})\"\n",
    "\n",
    "    final_lines = []\n",
    "    for line in lines:\n",
    "        matches = list(re.finditer(trigger, line))\n",
    "        if not matches:\n",
    "            final_lines.append(line.strip())\n",
    "            continue\n",
    "        \n",
    "        last_idx = 0\n",
    "        for m in matches:\n",
    "            start = m.start()\n",
    "            if line[last_idx:start].strip():\n",
    "                final_lines.append(line[last_idx:start].strip())\n",
    "            last_idx = start\n",
    "        \n",
    "        final_lines.append(line[last_idx:].strip())\n",
    "        \n",
    "    return final_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "adfd4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(lines):\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        res = re.sub(numering_items, '', line)\n",
    "        res = re.sub(numeric_pattern, '', res)\n",
    "        res = re.sub(english, ' ', res)\n",
    "        res = re.sub(numbers, '', res)\n",
    "        res = re.sub(empty_brackets, '', res)\n",
    "        res = re.sub(',', '،', res)\n",
    "        res = re.sub(';', '؛', res)\n",
    "        res = re.sub(r'\\?', '؟', res) \n",
    "        # res = re.sub(r'\\s+ا\\s*هـ?\\s+', ' ، ', res)\n",
    "        # res = re.sub(fr'\\((\\s*{stand_alone}\\s*)+\\)', f' {UNK_CHAR} ', res)\n",
    "        # res = re.sub(fr'(\\s*{stand_alone}\\s*)+', f' {UNK_CHAR} ', res) \n",
    "        res = re.sub(r'/', '', res)\n",
    "        res = re.sub(r'\\*', '', res)\n",
    "        res = re.sub(r'–', '-', res)\n",
    "        res = res.replace('\\u200f', '')\n",
    "        \n",
    "        res = clean_punctuation_sequence(res)\n",
    "        res = remove_unbalanced_brackets(res)\n",
    "        \n",
    "        res = re.sub(r\"\\s+\", \" \", res).strip()\n",
    "        new_lines.append(res)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a5f4bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path='../data/train.txt'):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return process_text(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e5cb3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAL == True:\n",
    "    train_lines = read_data('../data/val.txt')\n",
    "else:\n",
    "    train_lines = read_data('../data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b05fd503",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines = split_citations(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6cff3bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_lines = new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f3b2066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_lines = [remove_unbalanced_brackets(line) for line in new_lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d6a92",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af894c6",
   "metadata": {},
   "source": [
    "## TA code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "be4bee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../utils/arabic_letters.pickle' \n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    arabic_letters = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b25ee56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../utils/diacritic2id.pickle' \n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    diacritic2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0dea69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [' ', '،', ':', '؛', '!', '؟', '\"', \"'\", '«', '»', '(', ')', '[', ']', '{', '}', '-', '.']\n",
    "DIACRITICS_PATTERN = re.compile(r'[\\u064B-\\u0652]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b89c22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(text, labels, overlap=50, max_len=200):\n",
    "    assert len(text) == len(labels), \"Text and labels must be of the same length.\"\n",
    "    if len(text) <= max_len:\n",
    "        return [text], [labels]\n",
    "    \n",
    "    chunks = []\n",
    "    labels_chunks = []\n",
    "    \n",
    "    chunks.append(text[:max_len])\n",
    "    labels_chunks.append(labels[:max_len])\n",
    "    \n",
    "    current_start = 0\n",
    "    text_len = len(text)\n",
    "    \n",
    "    while True:\n",
    "        ideal_stride = max_len - overlap\n",
    "        \n",
    "        ideal_next_start = current_start + ideal_stride\n",
    "        \n",
    "        if ideal_next_start >= text_len:\n",
    "            break\n",
    "\n",
    "        found_next_start = -1\n",
    "        \n",
    "        search_limit = current_start  \n",
    "        \n",
    "        for i in range(ideal_next_start, search_limit, -1):\n",
    "            if i < text_len and text[i] == ' ':\n",
    "                found_next_start = i + 1 \n",
    "                break\n",
    "        \n",
    "        if found_next_start == -1:\n",
    "            found_next_start = ideal_next_start\n",
    "            \n",
    "        \n",
    "        actual_overlap = (current_start + max_len) - found_next_start\n",
    "        \n",
    "        if actual_overlap < 0:\n",
    "            actual_overlap = 0\n",
    "\n",
    "        next_chunk = text[found_next_start : found_next_start + max_len]\n",
    "        next_labels = labels[found_next_start : found_next_start + max_len]\n",
    "        \n",
    "        chunks.append(next_chunk)\n",
    "        labels_chunks.append(next_labels)\n",
    "        \n",
    "        current_start = found_next_start\n",
    "        \n",
    "        if current_start + max_len >= text_len:\n",
    "            break\n",
    "            \n",
    "    return (chunks, labels_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2a9dbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def slide_window(text, labels, overlap=50, max_len=200):\n",
    "#     assert len(text) == len(labels), \"Text and labels must be of the same length.\"\n",
    "#     if len(text) <= max_len:\n",
    "#         return [text], [labels]\n",
    "    \n",
    "#     text_chunks = []\n",
    "#     label_chunks = []\n",
    "    \n",
    "#     stride = max_len - overlap\n",
    "    \n",
    "#     for i in range(0, len(text), stride):\n",
    "#         t_chunk = text[i : i + max_len]\n",
    "        \n",
    "#         l_chunk = labels[i : i + max_len]\n",
    "        \n",
    "#         text_chunks.append(t_chunk)\n",
    "#         label_chunks.append(l_chunk)\n",
    "        \n",
    "#         if i + max_len >= len(text): \n",
    "#             break\n",
    "            \n",
    "#     return (text_chunks, label_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4841075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_and_diacritics(text):\n",
    "\n",
    "    letters = []\n",
    "    labels = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        char = text[i]\n",
    "        \n",
    "        if DIACRITICS_PATTERN.match(char):\n",
    "            if labels:\n",
    "                labels[-1] += char\n",
    "        \n",
    "        else:\n",
    "            letters.append(char)\n",
    "            labels.append(\"\") \n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    return \"\".join(letters), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "01798c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "cleaned_tashkeel = []\n",
    "\n",
    "for line in cleaned_lines:\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    text, tashkeel = split_text_and_diacritics(line)\n",
    "    if len(text) <= 5 and VAL == False:\n",
    "        continue\n",
    "    \n",
    "    cleaned_text.append(text)\n",
    "    cleaned_tashkeel.append(tashkeel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1480b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cleaned_text)):\n",
    "    assert len(cleaned_text[i]) == len(cleaned_tashkeel[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ed3c5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAL == False:\n",
    "    quantile = 0.99\n",
    "    lengths = [len(text) for text in cleaned_text]\n",
    "    MAX_LEN = int(sorted(lengths)[int(len(lengths) * quantile)])\n",
    "    \n",
    "else:\n",
    "    MAX_LEN = 807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e0841fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_lines = []\n",
    "chunked_labels = []\n",
    "for i, new_line in enumerate(cleaned_text):\n",
    "    chunks = slide_window(new_line, cleaned_tashkeel[i], overlap=50, max_len=MAX_LEN)\n",
    "    for i, chunk in enumerate(chunks[0]):\n",
    "        chunked_lines.append(chunk)\n",
    "        chunked_labels.append(chunks[1][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "aefebf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(chunked_lines)):\n",
    "    assert len(chunked_lines[i]) == len(chunked_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ce3555ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = set(arabic_letters) | set(punctuation) | set(diacritic2id.keys())\n",
    "allowed_chars.add(' ')  \n",
    "\n",
    "def replace_unknowns(text):\n",
    "    return \"\".join([char if char in allowed_chars else '?' for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1cbb971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunked_lines = [replace_unknowns(line) for line in chunked_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "06e2f7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char '?' not found in arabic_letters list.\n"
     ]
    }
   ],
   "source": [
    "char_set = {}\n",
    "for line in processed_chunked_lines:\n",
    "\n",
    "    for char in line:\n",
    "        if char in char_set:\n",
    "            continue\n",
    "        else:\n",
    "            if char not in arabic_letters and char not in punctuation and char not in diacritic2id.keys():\n",
    "                print(f\"Char {repr(char)} not found in arabic_letters list.\")   \n",
    "            char_set[char] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3dd4d0",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "33921a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(tokens_text, tokens_labels, max_len):\n",
    "    assert len(tokens_text) == len(tokens_labels), \"Tokens and labels must be of the same length.\"\n",
    "    if len(tokens_text) == max_len:\n",
    "        return (tokens_text[:max_len], tokens_labels[:max_len])\n",
    "    elif len(tokens_text) < max_len:\n",
    "        return (tokens_text + ['<PAD>'] * (max_len - len(tokens_text)), \n",
    "                tokens_labels + [''] * (max_len - len(tokens_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f60de732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_texts = []\n",
    "# padded_labels = []\n",
    "\n",
    "# for i, line in enumerate(chunked_lines):\n",
    "#     listed_line = []\n",
    "#     for char in line:\n",
    "#         listed_line.append(char) \n",
    "            \n",
    "#     padded_text, padded_label = pad_tokens(listed_line, chunked_labels[i], MAX_LEN)\n",
    "#     padded_texts.append(padded_text)\n",
    "#     padded_labels.append(padded_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ddab11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(padded_texts)):\n",
    "#     assert len(padded_texts[i]) == len(padded_labels[i]) == MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "49a6e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAL == True:\n",
    "    with open('../data/cleaned_val.txt', 'w', encoding='utf-8') as f:\n",
    "        for line in processed_chunked_lines:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    with open('../data/cleaned_tashkeel_val.txt', 'w', encoding='utf-8') as f:\n",
    "        for line in chunked_labels:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    # with open('../data/padded_val.pkl', 'wb') as f:\n",
    "    #     pickle.dump((padded_texts, padded_labels), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e8bfe",
   "metadata": {},
   "source": [
    "## Vocab building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5a0ea0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(padded_texts):\n",
    "    \n",
    "    word_set = {}\n",
    "    # i = 0\n",
    "    # for line in og_lines:\n",
    "    #     for word in line.split():\n",
    "    #         if word == UNK_CHAR:\n",
    "    #             continue\n",
    "    #         if word not in word_set:\n",
    "    #             word_set[word] = i\n",
    "    #             i += 1                \n",
    "\n",
    "    char2idx = {\n",
    "        '<PAD>' : 0,\n",
    "    }\n",
    "    i = 2\n",
    "    for text in padded_texts:\n",
    "        for char in text:\n",
    "            if char not in char2idx:\n",
    "                char2idx[char] = i\n",
    "                i += 1\n",
    "    \n",
    "    return char2idx, word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "368d3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx, word_vocab = build_vocab(processed_chunked_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "405ebde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = {v: k for k, v in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a159d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in arabic_letters:\n",
    "    if letter not in char2idx:\n",
    "        assert False, f\"Letter {repr(letter)} from arabic_letters not found in vocab.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8823b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in char2idx:\n",
    "    if letter not in arabic_letters and letter not in punctuation and letter != '<PAD>' and letter != UNK_CHAR:\n",
    "        print(f\"Letter {repr(letter)} from vocab not found in arabic_letters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6bb20002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in processed_chunked_lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "with open('../data/cleaned_tashkeel.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in chunked_labels:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "949150b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/padded_val.pkl', 'wb') as f:\n",
    "    for i in range(len(processed_chunked_lines)):\n",
    "        assert len(processed_chunked_lines[i]) == len(chunked_labels[i])\n",
    "    pickle.dump((processed_chunked_lines, chunked_labels), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b0bf8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/char2idx.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(char2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('../data/idx2char.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2char, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fb0f3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned_val.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in processed_chunked_lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "with open('../data/cleaned_tashkeel_val.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in chunked_labels:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fd929948",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {v: k for k, v in diacritic2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "57f3917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned_all.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(processed_chunked_lines)):\n",
    "        result_str = \"\"\n",
    "        for char, p_id in zip(processed_chunked_lines[i], chunked_labels[i]):\n",
    "            if char == '<PAD>': \n",
    "                break \n",
    "            \n",
    "            result_str += char + p_id\n",
    "        f.write(result_str + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad72d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
