{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b664b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 13:24:28.466295: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-04 13:24:28.466542: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 13:24:28.517075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-04 13:24:29.717693: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 13:24:29.717997: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3410bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pattern = r\"\\(\\s*\\d+\\s*/\\s*\\d+\\s*\\)\"\n",
    "english = r\"[a-zA-Z]\"\n",
    "numbers = r\"\\s*\\d+\\s*\"\n",
    "numering_items = r\"\\s*\\d+\\s*[-]\\s*\"\n",
    "empty_brackets = r'\\(\\s*\\)|\\[\\s*\\]|\\{\\s*\\}|<<\\s*>>|\"\\s*\"|\\'\\s*\\''\n",
    "stand_alone=r'(?<=\\s|\\^|\\(|\\[|\\{)[^\\(\\)\\[\\]\\{\\}\\.,،:;؛؟!\\-](?=\\s|$|\\]|\\)|\\})'\n",
    "UNK_CHAR = '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ca1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "kawloho_pattern = r\"(\\s*قَوْلُهُ\\s*)\"\n",
    "qala_variations = r\"(?:قَالَ|قَالَتْ|قُلْت|قَالُوا|قُلْنَا|أَقُولُ)\"\n",
    "qala_variations = r\"(?:قَالَ|قَالَتْ|قُلْت|قَالُوا|قُلْنَا|أَقُولُ)\"\n",
    "qala_pattern = rf\"(\\s*{qala_variations}\\s*:)\"\n",
    "INTAHA = r'\\s+ا\\s*هـ?\\s+'\n",
    "DIACRITICS_PATTERN = re.compile(r'[\\u064B-\\u0652]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b7ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zizo/Documents/NLP/Project/data/val.txt') as f:\n",
    "    lines = f.readlines()\n",
    "with open('/home/zizo/Documents/NLP/Project/utils/diacritics.pickle', 'rb') as f:\n",
    "    diacritic2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d719f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "diacritic2id = {\n",
    "    \"َ\": 0,\n",
    "    \"ً\": 1,\n",
    "    \"ُ\": 2,\n",
    "    \"ٌ\": 3,\n",
    "    \"ِ\": 4,\n",
    "    \"ٍ\": 5,\n",
    "    \"ْ\": 6,\n",
    "    \"ّ\": 7,\n",
    "    \"َّ\": 8,\n",
    "    \"ًّ\": 9,\n",
    "    \"ُّ\": 10,\n",
    "    \"ٌّ\": 11,\n",
    "    \"ِّ\": 12,\n",
    "    \"ٍّ\": 13,\n",
    "    \"\": 14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b003c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unbalanced_brackets(text):\n",
    "    pair_map = {')': '(', '}': '{', ']': '[', '>':'<', '»': '«', '\"':'\"', \"'\":\"'\"}\n",
    "    openers = set(['(', '{', '[', '<', '«', '\"', \"'\"])\n",
    "    \n",
    "    stack = [] \n",
    "    indices_to_remove = set()\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char in openers:\n",
    "            stack.append((char, i))\n",
    "        \n",
    "        elif char in pair_map:\n",
    "            if stack:\n",
    "                last_opener, _ = stack[-1]\n",
    "                if last_opener == pair_map[char]:\n",
    "                    stack.pop()\n",
    "                else:\n",
    "                    indices_to_remove.add(i)\n",
    "            else:\n",
    "                indices_to_remove.add(i)\n",
    "\n",
    "    for char, index in stack:\n",
    "        indices_to_remove.add(index)\n",
    "\n",
    "    return \"\".join([char for i, char in enumerate(text) if i not in indices_to_remove])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation_sequence(text):\n",
    "    collapsible = re.escape(\".,:;!?'\\\"/،؛؟\")\n",
    "    pattern = rf\"([{collapsible}])(?:\\s*\\1)+\"\n",
    "\n",
    "    return re.sub(pattern, r\"\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfef3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_citations_raw(line):\n",
    "    qal_list = [\n",
    "        \"قال\", \"قالت\", \"قالوا\", \"قلت\", \"قلنا\",\n",
    "        \"أقول\", \"يقول\", \"يقولون\", \"قيل\", \"يقال\"\n",
    "    ]\n",
    "\n",
    "    qal_regex = \"|\".join(qal_list)\n",
    "\n",
    "    qal_with_colon = rf\"(?:{qal_regex})\\s*[:：]\"\n",
    "\n",
    "    \n",
    "    qawloho_regex = r\"(?:و|ف)?قول(?:ه)?(?:\\s*تعالى)?\"\n",
    "\n",
    "    trigger = rf\"({qal_with_colon}|{qawloho_regex})\"\n",
    "\n",
    "    final_lines = []\n",
    "    matches = list(re.finditer(trigger, line))\n",
    "    \n",
    "    if not matches:\n",
    "        final_lines.append(line.strip())\n",
    "    else:\n",
    "        last_idx = 0\n",
    "        for m in matches:\n",
    "            start = m.start()\n",
    "            if line[last_idx:start]:\n",
    "                final_lines.append(line[last_idx:start])\n",
    "            last_idx = start\n",
    "        \n",
    "        final_lines.append(line[last_idx:])\n",
    "\n",
    "    return final_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d310fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_process(line):\n",
    "    res = re.sub(numering_items, '', line)\n",
    "    res = re.sub(numeric_pattern, '', res)\n",
    "    res = re.sub(english, ' ', res)\n",
    "    res = re.sub(numbers, '', res)\n",
    "    res = re.sub(empty_brackets, '', res)\n",
    "    res = re.sub(',', '،', res)\n",
    "    res = re.sub(';', '؛', res)\n",
    "    res = re.sub(r'\\?', '؟', res)\n",
    "    res = re.sub(r'/', '', res)\n",
    "    res = re.sub(r'\\*', '', res)\n",
    "    res = re.sub(r'–', '-', res)\n",
    "    res = res.replace('\\u200f', '')\n",
    "    \n",
    "\n",
    "    res = clean_punctuation_sequence(res)\n",
    "\n",
    "    res = remove_unbalanced_brackets(res)\n",
    "\n",
    "    res = re.sub(r\"\\s+\", \" \", res).strip()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ea2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window_raw(text, overlap=50, max_len=200):\n",
    "    if len(text) <= max_len:\n",
    "        return [text], []\n",
    "    \n",
    "    chunks = []\n",
    "    overlaps = []\n",
    "    \n",
    "    chunks.append(text[:max_len])\n",
    "    \n",
    "    current_start = 0\n",
    "    text_len = len(text)\n",
    "    \n",
    "    while True:\n",
    "        ideal_stride = max_len - overlap\n",
    "        \n",
    "        ideal_next_start = current_start + ideal_stride\n",
    "        \n",
    "        if ideal_next_start >= text_len:\n",
    "            break\n",
    "\n",
    "        found_next_start = -1\n",
    "        \n",
    "        search_limit = current_start  \n",
    "        \n",
    "        for i in range(ideal_next_start, search_limit, -1):\n",
    "            if i < text_len and text[i] == ' ':\n",
    "                found_next_start = i + 1 \n",
    "                break\n",
    "        \n",
    "        if found_next_start == -1:\n",
    "            found_next_start = ideal_next_start\n",
    "            \n",
    "        \n",
    "        actual_overlap = (current_start + max_len) - found_next_start\n",
    "        \n",
    "        if actual_overlap < 0:\n",
    "            actual_overlap = 0\n",
    "\n",
    "        next_chunk = text[found_next_start : found_next_start + max_len]\n",
    "        \n",
    "        chunks.append(next_chunk)\n",
    "        overlaps.append(actual_overlap)\n",
    "        \n",
    "        current_start = found_next_start\n",
    "        \n",
    "        if current_start + max_len >= text_len:\n",
    "            break\n",
    "            \n",
    "    return chunks, overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b7ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text_window(chunks, overlaps):\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "    \n",
    "    reconstructed_parts = [chunks[0]]\n",
    "    \n",
    "    for chunk, ov in zip(chunks[1:], overlaps):\n",
    "        reconstructed_parts.append(chunk[ov:])\n",
    "        \n",
    "    return \"\".join(reconstructed_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "037fc652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_and_diacritics(text):\n",
    "\n",
    "    letters = []\n",
    "    labels = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        char = text[i]\n",
    "\n",
    "        if DIACRITICS_PATTERN.match(char):\n",
    "            if labels:\n",
    "                labels[-1] += char\n",
    "        else:\n",
    "            letters.append(char)\n",
    "            labels.append(\"\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return \"\".join(letters), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b26b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(cleaned_lines):\n",
    "    \n",
    "#     test_x = []\n",
    "#     for sent in tqdm(cleaned_lines):\n",
    "#         sentence_vec = zizo_features(sent)\n",
    "#         test_x.append(sentence_vec)\n",
    "\n",
    "#     def test_generator():\n",
    "#         for features in test_x:\n",
    "#             yield features\n",
    "\n",
    "#     test_dataset = tf.data.Dataset.from_generator(\n",
    "#         test_generator,\n",
    "#         output_signature=tf.TensorSpec(shape=(None, INPUT_DIM), dtype=tf.float32)\n",
    "#     )\n",
    "\n",
    "#     test_dataset = test_dataset.padded_batch(\n",
    "#         32,\n",
    "#         padding_values=PADDING_INPUT \n",
    "#     )\n",
    "        \n",
    "#     predicted_tags = []\n",
    "    \n",
    "#     for batch_x in tqdm(test_dataset): \n",
    "#         batch_probs = model.predict_on_batch(batch_x)\n",
    "#         batch_pred_ids = np.argmax(batch_probs, axis=-1)\n",
    "        \n",
    "#         batch_size = batch_pred_ids.shape[0]\n",
    "        \n",
    "#         for k in range(batch_size):\n",
    "            \n",
    "#             valid_mask = np.any(batch_x[k] != PADDING_INPUT, axis=-1) \n",
    "           \n",
    "#             real_len = np.sum(valid_mask)\n",
    "            \n",
    "#             p_seq = batch_pred_ids[k][:real_len]\n",
    "            \n",
    "#             predicted_tags.append(p_seq)\n",
    "\n",
    "#     return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d2b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_predict():\n",
    "    all_text_chunks = []\n",
    "    all_recovery = []\n",
    "    all_overlaps = [] \n",
    "    length = 0\n",
    "    assertions = []\n",
    "    \n",
    "    with open('/home/zizo/Documents/NLP/Project/data/val.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "        \n",
    "        for line in file:\n",
    "            length += 1\n",
    "\n",
    "            line, _ = split_text_and_diacritics(line)\n",
    "            recovery = []\n",
    "            curr_chunks = []\n",
    "            curr_overlaps = [] \n",
    "            \n",
    "            cleaned = initial_process(line.strip())\n",
    "            assertions.append(cleaned)\n",
    "            \n",
    "            raw_segments = split_citations_raw(cleaned)\n",
    "\n",
    "            for seg in raw_segments:\n",
    "                t_chunks, t_overlaps = slide_window_raw(seg, overlap=50, max_len=807)\n",
    "                \n",
    "                for i, chunk in enumerate(t_chunks):\n",
    "                    recovery.append(i)\n",
    "                    curr_chunks.append(chunk)\n",
    "                \n",
    "               \n",
    "                curr_overlaps.extend(t_overlaps)\n",
    "\n",
    "            all_recovery.append(recovery)\n",
    "            all_text_chunks.append(curr_chunks)\n",
    "            all_overlaps.append(curr_overlaps) \n",
    "            \n",
    "    print(f\"Generated {len(all_text_chunks)} chunks.\")\n",
    "    return all_text_chunks, all_overlaps, all_recovery, length, assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a4855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2500 chunks.\n"
     ]
    }
   ],
   "source": [
    "all_text_chunks, all_overlaps, all_recovery, length, assertions = prepare_for_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d57ea152",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = []\n",
    "\n",
    "for i in range(len(all_recovery)):\n",
    "    res = ''\n",
    "    curr_chunks = []\n",
    "    curr_overlaps = []\n",
    "    \n",
    "    overlap_idx = 0 \n",
    "    \n",
    "    for j in range(len(all_recovery[i])):\n",
    "        recovery_id = all_recovery[i][j]\n",
    "        chunk = all_text_chunks[i][j]\n",
    "        \n",
    "        if recovery_id == 0:\n",
    "            if curr_chunks:\n",
    "                res += reconstruct_text_window(curr_chunks, curr_overlaps)\n",
    "            \n",
    "            curr_chunks = [chunk]\n",
    "            curr_overlaps = [] \n",
    "        \n",
    "        else:\n",
    "            curr_chunks.append(chunk)\n",
    "            \n",
    "            if overlap_idx < len(all_overlaps[i]):\n",
    "                curr_overlaps.append(all_overlaps[i][overlap_idx])\n",
    "                overlap_idx += 1\n",
    "\n",
    "    if curr_chunks:\n",
    "        res += reconstruct_text_window(curr_chunks, curr_overlaps)\n",
    "        \n",
    "    reconstructed.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7803db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zizo/Documents/NLP/Project/utils/arabic_letters.pickle', \"rb\") as file:\n",
    "    ARABIC_CHARS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e0a85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_only(text):\n",
    "    return \"\".join([char for char in text if char in ARABIC_CHARS or char == \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f9db818",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reconstructed)):\n",
    "    reconstructed[i] = arabic_only(reconstructed[i])\n",
    "    assertions[i] = arabic_only(assertions[i])\n",
    "    reconstructed[i] = re.sub(r\"\\s+\", \" \", reconstructed[i])\n",
    "    assertions[i] = re.sub(r\"\\s+\", \" \", assertions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8ea8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(reconstructed) == length\n",
    "for i in range(len(reconstructed)):\n",
    "    assert len(reconstructed[i].strip()) == len(assertions[i].strip()), print(reconstructed[i], \"\\n\", assertions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac19119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
