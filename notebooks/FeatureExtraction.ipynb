{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T13:03:07.107435Z",
          "iopub.status.busy": "2025-11-30T13:03:07.107161Z",
          "iopub.status.idle": "2025-11-30T13:04:40.379786Z",
          "shell.execute_reply": "2025-11-30T13:04:40.379002Z",
          "shell.execute_reply.started": "2025-11-30T13:03:07.107415Z"
        },
        "id": "BDvlUAFwC2Gc",
        "outputId": "f438574d-315a-432e-c316-6a8e003abc8e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
            "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lang-trans (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers arabert preprocess\n",
        "!pip install -q stanza\n",
        "!pip install -q gensim\n",
        "!pip install -q flair\n",
        "!pip install -q lang-trans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:54:40.138695Z",
          "iopub.status.busy": "2025-11-30T13:54:40.138053Z",
          "iopub.status.idle": "2025-11-30T13:54:40.143212Z",
          "shell.execute_reply": "2025-11-30T13:54:40.142495Z",
          "shell.execute_reply.started": "2025-11-30T13:54:40.138667Z"
        },
        "id": "kBF-Yq3-w3w9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "import stanza\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from gensim.models import FastText\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import CharacterEmbeddings, StackedEmbeddings\n",
        "from lang_trans.arabic import buckwalter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:54:42.781441Z",
          "iopub.status.busy": "2025-11-30T13:54:42.780817Z",
          "iopub.status.idle": "2025-11-30T13:54:42.784817Z",
          "shell.execute_reply": "2025-11-30T13:54:42.783933Z",
          "shell.execute_reply.started": "2025-11-30T13:54:42.781419Z"
        },
        "id": "ZiYFiY_7i50P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# loaded_weights = np.load('/content/drive/MyDrive/NLP Pro/features/arabic_char_embeddings_256.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:54:42.915206Z",
          "iopub.status.busy": "2025-11-30T13:54:42.914983Z",
          "iopub.status.idle": "2025-11-30T13:54:42.918573Z",
          "shell.execute_reply": "2025-11-30T13:54:42.91789Z",
          "shell.execute_reply.started": "2025-11-30T13:54:42.915189Z"
        },
        "id": "VTS1cNr8qSjK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# len(loaded_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:54:43.070371Z",
          "iopub.status.busy": "2025-11-30T13:54:43.06979Z",
          "iopub.status.idle": "2025-11-30T13:54:43.073957Z",
          "shell.execute_reply": "2025-11-30T13:54:43.073133Z",
          "shell.execute_reply.started": "2025-11-30T13:54:43.070348Z"
        },
        "id": "ElFIn4g7HqGG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "padded_path = \"/kaggle/input/arabicia/padded.pkl\"\n",
        "val_path = \"/kaggle/input/arabicia/padded_val.pkl\"\n",
        "\n",
        "diacritic2id_path = \"/kaggle/input/arabicia/diacritic2id.json\"\n",
        "idx2char_path = \"/kaggle/input/arabicia/idx2char.json\"\n",
        "char2idx_path = \"/kaggle/input/arabicia/char2idx.json\"\n",
        "\n",
        "char_embeddings_path = \"/kaggle/input/char-embeddings/keras/default/1/embedding_matrix.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:54:43.191768Z",
          "iopub.status.busy": "2025-11-30T13:54:43.191484Z",
          "iopub.status.idle": "2025-11-30T13:54:43.196662Z",
          "shell.execute_reply": "2025-11-30T13:54:43.195888Z",
          "shell.execute_reply.started": "2025-11-30T13:54:43.191748Z"
        },
        "id": "bwSWaux7HqGI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_data_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        X_raw, y_raw = pickle.load(f)\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for text_seq, label_seq in zip(X_raw, y_raw):\n",
        "        x_ids = [c for c in text_seq]\n",
        "        y_ids = [t for t in label_seq]\n",
        "\n",
        "        X.append(x_ids)\n",
        "        y.append(y_ids)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:54:43.262798Z",
          "iopub.status.busy": "2025-11-30T13:54:43.262242Z",
          "iopub.status.idle": "2025-11-30T13:54:43.270748Z",
          "shell.execute_reply": "2025-11-30T13:54:43.270027Z",
          "shell.execute_reply.started": "2025-11-30T13:54:43.262769Z"
        },
        "id": "t9rDezEDHqGJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with open(diacritic2id_path, 'r', encoding='utf-8') as f:\n",
        "    diacritic2id = json.load(f)\n",
        "\n",
        "id2label = {v: k for k, v in diacritic2id.items()}\n",
        "\n",
        "with open(char2idx_path, 'r', encoding='utf-8') as f:\n",
        "    char2idx = json.load(f)\n",
        "\n",
        "with open(idx2char_path, 'r', encoding='utf-8') as f:\n",
        "    idx2char = json.load(f)\n",
        "\n",
        "idx2char['1'] = '?'\n",
        "value = char2idx.pop('\\uFFFD')\n",
        "char2idx['?'] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:54:43.419261Z",
          "iopub.status.busy": "2025-11-30T13:54:43.418548Z",
          "iopub.status.idle": "2025-11-30T13:55:15.954825Z",
          "shell.execute_reply": "2025-11-30T13:55:15.95399Z",
          "shell.execute_reply.started": "2025-11-30T13:54:43.419231Z"
        },
        "id": "ceDT-9CkHqGL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sentences, tashkeel_sequences = load_data_pickle(padded_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:15.956296Z",
          "iopub.status.busy": "2025-11-30T13:55:15.956045Z",
          "iopub.status.idle": "2025-11-30T13:55:15.96146Z",
          "shell.execute_reply": "2025-11-30T13:55:15.960911Z",
          "shell.execute_reply.started": "2025-11-30T13:55:15.95627Z"
        },
        "id": "9HAc-CqAHqGM",
        "outputId": "4600a78d-a790-4f12-e42d-6cd93451764b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences: 80254\n",
            "Example sentence: Ùˆ\n",
            "Example tashkeel sequence: Ù\n",
            "Example char2idx size: 54\n",
            "Example diacritic2id size: 15 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Sentences:\", len(sentences))\n",
        "print(\"Example sentence:\", sentences[0][0])\n",
        "print(\"Example tashkeel sequence:\", tashkeel_sequences[0][0])\n",
        "print(\"Example char2idx size:\", len(char2idx))\n",
        "print(\"Example diacritic2id size:\", len(diacritic2id), diacritic2id[tashkeel_sequences[0][0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bumtkhM3xjHx"
      },
      "source": [
        "# **Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:15.962277Z",
          "iopub.status.busy": "2025-11-30T13:55:15.962105Z",
          "iopub.status.idle": "2025-11-30T13:55:16.602772Z",
          "shell.execute_reply": "2025-11-30T13:55:16.602148Z",
          "shell.execute_reply.started": "2025-11-30T13:55:15.962264Z"
        },
        "id": "jf-1WIoJw67d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "arabert_model_name = \"aubmindlab/bert-base-arabertv02\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(arabert_model_name)\n",
        "bert_model = AutoModel.from_pretrained(arabert_model_name)\n",
        "bert_model.eval()\n",
        "arabert_prep = ArabertPreprocessor(model_name=arabert_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:16.604822Z",
          "iopub.status.busy": "2025-11-30T13:55:16.60457Z",
          "iopub.status.idle": "2025-11-30T13:55:16.608183Z",
          "shell.execute_reply": "2025-11-30T13:55:16.607575Z",
          "shell.execute_reply.started": "2025-11-30T13:55:16.604805Z"
        },
        "id": "srcikqOusI6P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Model name (AraELECTRA large or small)\n",
        "# MODEL_NAME = \"aubmindlab/araelectra-base-discriminator\"\n",
        "\n",
        "# # Load tokenizer and model\n",
        "# electra_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# electra_model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# # Set to evaluation mode\n",
        "# electra_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:16.610068Z",
          "iopub.status.busy": "2025-11-30T13:55:16.60983Z",
          "iopub.status.idle": "2025-11-30T13:55:16.623467Z",
          "shell.execute_reply": "2025-11-30T13:55:16.622643Z",
          "shell.execute_reply.started": "2025-11-30T13:55:16.610053Z"
        },
        "id": "uLxJruXQw9T1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# stanza.download('ar')\n",
        "# nlp = stanza.Pipeline(\"ar\", processors=\"tokenize,pos\", use_gpu=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:16.624342Z",
          "iopub.status.busy": "2025-11-30T13:55:16.62418Z",
          "iopub.status.idle": "2025-11-30T13:55:16.636954Z",
          "shell.execute_reply": "2025-11-30T13:55:16.636286Z",
          "shell.execute_reply.started": "2025-11-30T13:55:16.624329Z"
        },
        "id": "gakXZpu-w_g2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# vocab_size = len(char2idx) + 1\n",
        "# embedding_dim = 128\n",
        "# char_embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "#                               embedding_dim=embedding_dim,\n",
        "#                               padding_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:16.637982Z",
          "iopub.status.busy": "2025-11-30T13:55:16.637751Z",
          "iopub.status.idle": "2025-11-30T13:55:16.647728Z",
          "shell.execute_reply": "2025-11-30T13:55:16.647071Z",
          "shell.execute_reply.started": "2025-11-30T13:55:16.637958Z"
        },
        "id": "NXFuVr9NHqGQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def remove_pads(sentence, tashkeel):\n",
        "    chars = []\n",
        "    tashkeelat = []\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i] != '<PAD>':\n",
        "            chars.append(sentence[i])\n",
        "            tashkeelat.append(tashkeel[i])\n",
        "    sentence_text = \"\".join(chars)\n",
        "    text = sentence_text.replace('\\uFFFD', '?')\n",
        "    return text, tashkeelat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:16.648626Z",
          "iopub.status.busy": "2025-11-30T13:55:16.648394Z",
          "iopub.status.idle": "2025-11-30T13:55:42.419554Z",
          "shell.execute_reply": "2025-11-30T13:55:42.418774Z",
          "shell.execute_reply.started": "2025-11-30T13:55:16.648604Z"
        },
        "id": "YrntzlOIHqGR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "new_sentences = []\n",
        "new_tashkeel = []\n",
        "for i in range(len(sentences)):\n",
        "    text, tashkeel = remove_pads(sentences[i], tashkeel_sequences[i])\n",
        "    new_sentences.append(text)\n",
        "    new_tashkeel.append(tashkeel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.420579Z",
          "iopub.status.busy": "2025-11-30T13:55:42.420397Z",
          "iopub.status.idle": "2025-11-30T13:55:42.632087Z",
          "shell.execute_reply": "2025-11-30T13:55:42.631335Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.420565Z"
        },
        "id": "5kWOGtN7HqGR",
        "outputId": "e93206a5-2f30-4361-a6de-4a9e90a972b1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ GPU Detected: Tesla T4\n",
            "   Memory Usage: 0.51 GB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"ğŸš€ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory Usage: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "bert_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X6c2CrhxC-K"
      },
      "source": [
        "# **AraBERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.634832Z",
          "iopub.status.busy": "2025-11-30T13:55:42.634546Z",
          "iopub.status.idle": "2025-11-30T13:55:42.639195Z",
          "shell.execute_reply": "2025-11-30T13:55:42.638663Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.634814Z"
        },
        "id": "wK9B3zb7FwXc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_arabert_embeddings(sentence: str):\n",
        "\n",
        "    tokens = bert_tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = bert_model(**tokens)\n",
        "\n",
        "    emb = output.last_hidden_state.squeeze(0).cpu()\n",
        "    token_list = bert_tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
        "\n",
        "    return emb.numpy(), token_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.640199Z",
          "iopub.status.busy": "2025-11-30T13:55:42.639884Z",
          "iopub.status.idle": "2025-11-30T13:55:42.66327Z",
          "shell.execute_reply": "2025-11-30T13:55:42.662592Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.640174Z"
        },
        "id": "x9jwa387F39i",
        "outputId": "07bec647-1353-4593-9621-7a74a4f33028",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape: <class 'numpy.ndarray'>\n",
            "Tokens: ['[CLS]', 'ÙˆÙ„Ùˆ', 'Ø¬Ù…Ø¹', 'Ø«Ù…', 'Ø¹Ù„Ù…', 'ØªØ±Ùƒ', 'Ø±ÙƒÙ†', 'Ù…Ù†', 'Ø§Ù„Ø£ÙˆÙ„Ù‰', 'Ø¨Ø·Ù„', '##ØªØ§', 'ÙˆÙŠØ¹ÙŠØ¯', '##Ù‡Ù…Ø§', 'Ø¬Ø§Ù…Ø¹Ø§', 'ØŒ', 'Ø£Ùˆ', 'Ù…Ù†', 'Ø§Ù„Ø«Ø§Ù†ÙŠØ©', 'ØŒ', 'ÙØ¥Ù†', 'Ù„Ù…', 'ÙŠØ·Ù„', 'ØªØ¯Ø§Ø±Ùƒ', 'ØŒ', 'ÙˆØ¥Ù„Ø§', 'ÙØ¨', '##Ø§Ø·', '##Ù„Ø©', 'ÙˆÙ„Ø§', 'Ø¬Ù…Ø¹', 'ØŒ', 'ÙˆÙ„Ùˆ', 'Ø¬Ù‡Ù„', 'Ø£Ø¹Ø§Ø¯', '##Ù‡Ù…Ø§', 'Ù„ÙˆÙ‚Øª', '##ÙŠÙ‡Ù…', '##Ø§', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "emb, toks = get_arabert_embeddings(new_sentences[0])\n",
        "print(\"Embedding shape:\", type(emb))\n",
        "print(\"Tokens:\", toks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.664217Z",
          "iopub.status.busy": "2025-11-30T13:55:42.664019Z",
          "iopub.status.idle": "2025-11-30T13:55:42.667338Z",
          "shell.execute_reply": "2025-11-30T13:55:42.666729Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.664192Z"
        },
        "id": "xqkkrGEbz9_H",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# tokens = []\n",
        "# bert_embeddings = []\n",
        "\n",
        "# for sent in sentences:\n",
        "#   emb, tok = get_arabert_embeddings(sentences,device)\n",
        "#   bert_embeddings.append(emb)\n",
        "#   tokens.append(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtkP6cyjxMSc"
      },
      "source": [
        "# **POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.668391Z",
          "iopub.status.busy": "2025-11-30T13:55:42.668085Z",
          "iopub.status.idle": "2025-11-30T13:55:42.679941Z",
          "shell.execute_reply": "2025-11-30T13:55:42.679337Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.668363Z"
        },
        "id": "yGd7NQvAF6_L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def extract_pos_tags(sentence: str):\n",
        "#     doc = nlp(sentence)\n",
        "#     pos_tags = []\n",
        "#     for sent_obj in doc.sentences:\n",
        "#         for word in sent_obj.words:\n",
        "#             pos_tags.append(word.upos)\n",
        "#     return pos_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.681037Z",
          "iopub.status.busy": "2025-11-30T13:55:42.680727Z",
          "iopub.status.idle": "2025-11-30T13:55:42.69002Z",
          "shell.execute_reply": "2025-11-30T13:55:42.689325Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.681014Z"
        },
        "id": "Zm8n8ObSGOuD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print(extract_pos_tags(sentences[0])[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIMF6K9DxV0N"
      },
      "source": [
        "# **Char Level Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.69137Z",
          "iopub.status.busy": "2025-11-30T13:55:42.690973Z",
          "iopub.status.idle": "2025-11-30T13:55:42.702198Z",
          "shell.execute_reply": "2025-11-30T13:55:42.701592Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.691353Z"
        },
        "id": "2xW-oQr5GvSI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def char_encode(sentence: str):\n",
        "    return [char2idx[c] for c in sentence]\n",
        "\n",
        "def char_embed(sentence: str):\n",
        "    ids = torch.tensor(char_encode(sentence))\n",
        "    return char_embedding(ids).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.703889Z",
          "iopub.status.busy": "2025-11-30T13:55:42.703592Z",
          "iopub.status.idle": "2025-11-30T13:55:42.712517Z",
          "shell.execute_reply": "2025-11-30T13:55:42.711804Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.703874Z"
        },
        "id": "bgt4aYdWxdnP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print(\"Char embedding:\", char_embed('Ø±'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yyc2AD0QgCz"
      },
      "source": [
        "# **Fast Text Word Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.71355Z",
          "iopub.status.busy": "2025-11-30T13:55:42.713315Z",
          "iopub.status.idle": "2025-11-30T13:55:42.722792Z",
          "shell.execute_reply": "2025-11-30T13:55:42.721915Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.71353Z"
        },
        "id": "iEP_bP_3Qk4U",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def get_arabic_tokens(corpus):\n",
        "#     data = [sentence.split() for sentence in corpus]\n",
        "#     return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.724401Z",
          "iopub.status.busy": "2025-11-30T13:55:42.723702Z",
          "iopub.status.idle": "2025-11-30T13:55:42.735834Z",
          "shell.execute_reply": "2025-11-30T13:55:42.735137Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.724378Z"
        },
        "id": "RaSq0klkSIEr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def train_fasttext_arabic(corpus, embedding_size=100, window_size=5, min_count=3, epochs=50, model_path=\"./models/ft_arabic_model\"):\n",
        "#     \"\"\"\n",
        "#     Train a FastText model on raw Arabic tokens\n",
        "#     \"\"\"\n",
        "#     data = get_arabic_tokens(corpus)\n",
        "\n",
        "#     # Initialize FastText model\n",
        "#     ft_model = FastText(\n",
        "#         vector_size=embedding_size,\n",
        "#         window=window_size,\n",
        "#         min_count=min_count,\n",
        "#         workers=4,\n",
        "#         sg=1  # Skip-gram\n",
        "#     )\n",
        "#     ft_model.build_vocab(corpus_iterable=data)\n",
        "#     ft_model.train(corpus_iterable=data, total_examples=len(data), epochs=epochs)\n",
        "\n",
        "#     # Save model\n",
        "#     os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "#     ft_model.save(model_path)\n",
        "#     print(f\"âœ… FastText Arabic model saved at {model_path}\")\n",
        "\n",
        "#     return ft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.736882Z",
          "iopub.status.busy": "2025-11-30T13:55:42.736645Z",
          "iopub.status.idle": "2025-11-30T13:55:42.744828Z",
          "shell.execute_reply": "2025-11-30T13:55:42.744103Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.736861Z"
        },
        "id": "sHXgelIpSKeN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def extract_fasttext_embeddings_arabic(corpus, ft_model):\n",
        "#     \"\"\"\n",
        "#     Extract FastText embeddings for each word in the corpus\n",
        "#     Returns a list of sentences, where each sentence is a list of word vectors\n",
        "#     \"\"\"\n",
        "#     data = get_arabic_tokens(corpus)\n",
        "#     all_embeddings = []\n",
        "\n",
        "#     for sentence in data:\n",
        "#         sentence_embeddings = []\n",
        "#         for word in sentence:\n",
        "#             vec = ft_model.wv[word]  # FastText handles OOV words via subword info\n",
        "#             sentence_embeddings.append(vec)\n",
        "#         all_embeddings.append(sentence_embeddings)\n",
        "\n",
        "#     return all_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.745832Z",
          "iopub.status.busy": "2025-11-30T13:55:42.745569Z",
          "iopub.status.idle": "2025-11-30T13:55:42.75673Z",
          "shell.execute_reply": "2025-11-30T13:55:42.756123Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.745811Z"
        },
        "id": "4LwzjAfsSMUC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ft_model = train_fasttext_arabic(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.757772Z",
          "iopub.status.busy": "2025-11-30T13:55:42.757519Z",
          "iopub.status.idle": "2025-11-30T13:55:42.766698Z",
          "shell.execute_reply": "2025-11-30T13:55:42.766085Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.757754Z"
        },
        "id": "jGlDSsOpUY98",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# extract_fasttext_embeddings_arabic(sentences[0],ft_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.767555Z",
          "iopub.status.busy": "2025-11-30T13:55:42.767335Z",
          "iopub.status.idle": "2025-11-30T13:55:42.775119Z",
          "shell.execute_reply": "2025-11-30T13:55:42.774516Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.767535Z"
        },
        "id": "l-uUMYzQUkLr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !cp -r \"./models/\" '/content/drive/MyDrive/NLP/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oheTTwPHqGW"
      },
      "source": [
        "# Custom Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.775954Z",
          "iopub.status.busy": "2025-11-30T13:55:42.775774Z",
          "iopub.status.idle": "2025-11-30T13:55:42.788516Z",
          "shell.execute_reply": "2025-11-30T13:55:42.787835Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.775934Z"
        },
        "id": "-IGXyNceHqGW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "custom_char_embedding = np.load(char_embeddings_path)\n",
        "\n",
        "def extract_custom_char_embeddings(char):\n",
        "    return custom_char_embedding[char2idx[char]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Jy9zRXWNNZ"
      },
      "source": [
        "# **FLAIR Char Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.789911Z",
          "iopub.status.busy": "2025-11-30T13:55:42.789368Z",
          "iopub.status.idle": "2025-11-30T13:55:42.792862Z",
          "shell.execute_reply": "2025-11-30T13:55:42.792151Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.789893Z"
        },
        "id": "n3qPm7Y-WqNp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# char_embedding = CharacterEmbeddings()\n",
        "\n",
        "# def extract_char_embeddings(sentence_text, embedding_model=None):\n",
        "\n",
        "#     if embedding_model is None:\n",
        "#         embedding_model = char_embedding\n",
        "#     sentence = Sentence(sentence_text)\n",
        "#     embedding_model.embed(sentence)\n",
        "\n",
        "#     return [token.embedding.detach().cpu() for token in sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.793877Z",
          "iopub.status.busy": "2025-11-30T13:55:42.793558Z",
          "iopub.status.idle": "2025-11-30T13:55:42.80357Z",
          "shell.execute_reply": "2025-11-30T13:55:42.802916Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.793859Z"
        },
        "id": "IeDJXVC0XPZu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# extract_char_embeddings(sentence_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyNRERusr28X"
      },
      "source": [
        "# **AraELECTRA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.804462Z",
          "iopub.status.busy": "2025-11-30T13:55:42.804251Z",
          "iopub.status.idle": "2025-11-30T13:55:42.813249Z",
          "shell.execute_reply": "2025-11-30T13:55:42.812668Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.804439Z"
        },
        "id": "_ibJzfwsr8Ab",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def get_araelectra_embeddings(sentence, model, tokenizer, device=\"cpu\"):\n",
        "#     \"\"\"\n",
        "#     Get token-level embeddings from AraELECTRA\n",
        "#     Returns a list of sentence embeddings (list of token embeddings)\n",
        "#     \"\"\"\n",
        "#     model.to(device)\n",
        "#     inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "#     # Get outputs (last hidden state)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#         last_hidden_state = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "#     # Remove batch dimension and convert to list of embeddings per token\n",
        "#     token_embeddings = last_hidden_state.squeeze(0)  # [seq_len, hidden_size]\n",
        "#     return token_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.814174Z",
          "iopub.status.busy": "2025-11-30T13:55:42.813961Z",
          "iopub.status.idle": "2025-11-30T13:55:42.825015Z",
          "shell.execute_reply": "2025-11-30T13:55:42.824384Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.814153Z"
        },
        "id": "vQEcZy5Xr_0v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Example: using your Buckwalter-transliterated sentences or cleaned Arabic\n",
        "# ara_embeddings = get_araelectra_embeddings(sentences[0], electra_model, electra_tokenizer, device)\n",
        "\n",
        "# print(\"Number of tokens in first sentence:\", ara_embeddings.shape[0])\n",
        "# print(\"Embedding dimension:\", ara_embeddings.shape[1])\n",
        "# print(ara_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROP4TmiaY2W1"
      },
      "source": [
        "# **Buckwalter Translation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.8289Z",
          "iopub.status.busy": "2025-11-30T13:55:42.828643Z",
          "iopub.status.idle": "2025-11-30T13:55:42.835463Z",
          "shell.execute_reply": "2025-11-30T13:55:42.834812Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.828885Z"
        },
        "id": "QNz1kuPqY698",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def get_buckwalter_translation(sentence):\n",
        "#   return buckwalter.transliterate(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.837073Z",
          "iopub.status.busy": "2025-11-30T13:55:42.836315Z",
          "iopub.status.idle": "2025-11-30T13:55:42.845806Z",
          "shell.execute_reply": "2025-11-30T13:55:42.845092Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.837042Z"
        },
        "id": "P13AKn2ZeEQ4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print(\"Original sentence:\", sentences[0])\n",
        "# print(\"Buckwalter transliterated:\", get_buckwalter_translation(sentences[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.84689Z",
          "iopub.status.busy": "2025-11-30T13:55:42.84666Z",
          "iopub.status.idle": "2025-11-30T13:55:42.855675Z",
          "shell.execute_reply": "2025-11-30T13:55:42.854959Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.846864Z"
        },
        "id": "DetdVFPuh_Wf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# buckwalter_sentences = []\n",
        "# for s in sentences:\n",
        "#   buckwalter_sentences.append(get_buckwalter_translation(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Z97z0vgOiN"
      },
      "source": [
        "# **TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.856861Z",
          "iopub.status.busy": "2025-11-30T13:55:42.856566Z",
          "iopub.status.idle": "2025-11-30T13:55:42.864457Z",
          "shell.execute_reply": "2025-11-30T13:55:42.863841Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.856843Z"
        },
        "id": "3XCCzGl_gSCN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def tf_idf_features(sentences, save_path=\"models/tf_idf_buckwalter.csv\"):\n",
        "#     os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "#     vectorizer = TfidfVectorizer(lowercase=False)  # Don't lowercase Buckwalter\n",
        "#     tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "#     feature_names = vectorizer.get_feature_names_out()\n",
        "#     df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "#     df_tfidf.to_csv(save_path, index=False)\n",
        "\n",
        "#     print(f\"âœ… TF-IDF features saved at {save_path}\")\n",
        "#     return df_tfidf, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.865431Z",
          "iopub.status.busy": "2025-11-30T13:55:42.865237Z",
          "iopub.status.idle": "2025-11-30T13:55:42.874785Z",
          "shell.execute_reply": "2025-11-30T13:55:42.874173Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.865414Z"
        },
        "id": "zO6MKeS7hPLI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# tfidf_df, tfidf_vectorizer = tf_idf_features(buckwalter_sentences)\n",
        "# print(\"TF-IDF shape:\", tfidf_df.shape)\n",
        "# print(\"Example TF-IDF features:\", tfidf_df.columns[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_jJ_n46hGCg"
      },
      "source": [
        "# **BOW**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.875816Z",
          "iopub.status.busy": "2025-11-30T13:55:42.875561Z",
          "iopub.status.idle": "2025-11-30T13:55:42.888147Z",
          "shell.execute_reply": "2025-11-30T13:55:42.887425Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.875793Z"
        },
        "id": "wMMLdprjhJYy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def bow_features(sentences, save_path=\"models/bow_buckwalter.csv\"):\n",
        "#     os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "#     vectorizer = CountVectorizer()\n",
        "#     bow_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "#     feature_names = vectorizer.get_feature_names_out()\n",
        "#     df_bow = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
        "#     df_bow.to_csv(save_path, index=False)\n",
        "\n",
        "#     print(f\"âœ… Bag-of-Words features saved at {save_path}\")\n",
        "#     return df_bow, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.889476Z",
          "iopub.status.busy": "2025-11-30T13:55:42.888858Z",
          "iopub.status.idle": "2025-11-30T13:55:42.897861Z",
          "shell.execute_reply": "2025-11-30T13:55:42.897175Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.889458Z"
        },
        "id": "iN7qx5dIhU0v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# bow_df, bow_vectorizer = bow_features(buckwalter_sentences)\n",
        "# print(\"BoW shape:\", bow_df.shape)\n",
        "# print(\"Example BoW features:\", bow_df.columns[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IycfI9rexkWw"
      },
      "source": [
        "# **Labeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.89896Z",
          "iopub.status.busy": "2025-11-30T13:55:42.898652Z",
          "iopub.status.idle": "2025-11-30T13:55:42.907661Z",
          "shell.execute_reply": "2025-11-30T13:55:42.906916Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.898883Z"
        },
        "id": "1-WMKgFlfs9W",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_tashkeel_sequence(index: int):\n",
        "    return new_tashkeel[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.908806Z",
          "iopub.status.busy": "2025-11-30T13:55:42.908457Z",
          "iopub.status.idle": "2025-11-30T13:55:42.918157Z",
          "shell.execute_reply": "2025-11-30T13:55:42.91752Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.90879Z"
        },
        "id": "i4jasD5cxpYJ",
        "outputId": "4d6452e3-f3e6-4ba2-d3af-629a49dd36ec",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ù', 'Ù', 'Ù’', '', 'Ù', 'Ù', 'Ù', '', 'Ù', 'Ù‘Ù']\n"
          ]
        }
      ],
      "source": [
        "print(get_tashkeel_sequence(0)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.919139Z",
          "iopub.status.busy": "2025-11-30T13:55:42.918915Z",
          "iopub.status.idle": "2025-11-30T13:55:42.929483Z",
          "shell.execute_reply": "2025-11-30T13:55:42.928582Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.919113Z"
        },
        "id": "CIg4SMwjHqGg",
        "outputId": "67290270-b32f-4f22-aa65-bf25628e2452",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ÙˆÙÙ„ÙÙˆÙ’ Ø¬ÙÙ…ÙØ¹Ù Ø«ÙÙ…Ù‘Ù Ø¹ÙÙ„ÙÙ…Ù '"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_str = ''\n",
        "for i in range(15):\n",
        "    result_str += new_sentences[0][i] + new_tashkeel[0][i]\n",
        "result_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utx-TAHkxrlu"
      },
      "source": [
        "# **Full Feature Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.930377Z",
          "iopub.status.busy": "2025-11-30T13:55:42.93013Z",
          "iopub.status.idle": "2025-11-30T13:55:42.944049Z",
          "shell.execute_reply": "2025-11-30T13:55:42.943205Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.930356Z"
        },
        "id": "81fWJ9IN4mrp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def tokens_to_word_embeddings(tokens, embeddings):\n",
        "    word_embeddings = []\n",
        "    current_word_embs = []\n",
        "\n",
        "    for token, emb in zip(tokens, embeddings):\n",
        "        emb_tensor = torch.tensor(emb) if isinstance(emb, np.ndarray) else emb\n",
        "\n",
        "        if token.startswith(\"##\"):\n",
        "            current_word_embs.append(emb_tensor)\n",
        "        else:\n",
        "            if current_word_embs:\n",
        "                word_embeddings.append(torch.mean(torch.stack(current_word_embs), dim=0))\n",
        "            current_word_embs = [emb_tensor]\n",
        "\n",
        "    if current_word_embs:\n",
        "        word_embeddings.append(torch.mean(torch.stack(current_word_embs), dim=0))\n",
        "\n",
        "    return torch.stack(word_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.945094Z",
          "iopub.status.busy": "2025-11-30T13:55:42.944879Z",
          "iopub.status.idle": "2025-11-30T13:55:42.959895Z",
          "shell.execute_reply": "2025-11-30T13:55:42.957987Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.945073Z"
        },
        "id": "k1SrEJNRwiFy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def zizo_features(sentence: str,\n",
        "                  sent_index: int,\n",
        "                  arabert_model=None,\n",
        "                  arabert_tokenizer=None,\n",
        "                  fasttext_model=None):\n",
        "\n",
        "    sentence_vec = []\n",
        "\n",
        "    tashkeel = get_tashkeel_sequence(sent_index)\n",
        "\n",
        "    arabert_emb, tokens = get_arabert_embeddings(sentence)\n",
        "    final_arabert_emb = tokens_to_word_embeddings(tokens, arabert_emb)\n",
        "\n",
        "    words_raw = sentence.split()\n",
        "    word_idx = 0\n",
        "    char_in_word_idx = 0\n",
        "\n",
        "    emb_dim = final_arabert_emb[0].shape[0]\n",
        "\n",
        "    for i, char in enumerate(sentence):\n",
        "\n",
        "        char_emb = extract_custom_char_embeddings(char)\n",
        "        char_emb_array = np.array(char_emb).flatten()\n",
        "\n",
        "        if char == ' ':\n",
        "            bert_vec = np.zeros(emb_dim)\n",
        "\n",
        "        else:\n",
        "            bert_vec = final_arabert_emb[word_idx]\n",
        "            if isinstance(bert_vec, torch.Tensor):\n",
        "                bert_vec = bert_vec.numpy()\n",
        "\n",
        "            char_in_word_idx += 1\n",
        "\n",
        "            if char_in_word_idx == len(words_raw[word_idx]):\n",
        "                word_idx += 1\n",
        "                char_in_word_idx = 0\n",
        "\n",
        "        char_vector = np.concatenate([bert_vec, char_emb_array])\n",
        "        sentence_vec.append(char_vector)\n",
        "\n",
        "    return sentence_vec, tashkeel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:42.960871Z",
          "iopub.status.busy": "2025-11-30T13:55:42.960481Z",
          "iopub.status.idle": "2025-11-30T13:55:43.007552Z",
          "shell.execute_reply": "2025-11-30T13:55:43.006368Z",
          "shell.execute_reply.started": "2025-11-30T13:55:42.960835Z"
        },
        "id": "U3XuJ3t8yf4R",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "f,t = zizo_features(new_sentences[0], 0,bert_model,bert_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:43.008902Z",
          "iopub.status.busy": "2025-11-30T13:55:43.008593Z",
          "iopub.status.idle": "2025-11-30T13:55:43.015914Z",
          "shell.execute_reply": "2025-11-30T13:55:43.01408Z",
          "shell.execute_reply.started": "2025-11-30T13:55:43.008876Z"
        },
        "id": "DBNHktH48X7s",
        "outputId": "fe338f80-3c99-4743-b2f8-07ca02aefead",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "137"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:52.068541Z",
          "iopub.status.busy": "2025-11-30T13:55:52.067958Z",
          "iopub.status.idle": "2025-11-30T13:55:52.073064Z",
          "shell.execute_reply": "2025-11-30T13:55:52.072252Z",
          "shell.execute_reply.started": "2025-11-30T13:55:52.068518Z"
        },
        "id": "qhTgC5e18Zyv",
        "outputId": "a95e2f00-3b06-427d-f00d-bf9d9f0af15b",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(f[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:52.660443Z",
          "iopub.status.busy": "2025-11-30T13:55:52.659861Z",
          "iopub.status.idle": "2025-11-30T13:55:52.665109Z",
          "shell.execute_reply": "2025-11-30T13:55:52.664339Z",
          "shell.execute_reply.started": "2025-11-30T13:55:52.660419Z"
        },
        "id": "qp_pnv9K8e70",
        "outputId": "2a8d7293-2f8b-4f19-d6ae-62ec8dc8e688",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "137"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(new_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:53.410588Z",
          "iopub.status.busy": "2025-11-30T13:55:53.409893Z",
          "iopub.status.idle": "2025-11-30T13:55:53.415089Z",
          "shell.execute_reply": "2025-11-30T13:55:53.41429Z",
          "shell.execute_reply.started": "2025-11-30T13:55:53.410562Z"
        },
        "id": "kYooecMf-zWv",
        "outputId": "417d7fa1-d995-4eca-ed31-0878e72c5c2a",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "137"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_fLP3GDHqGi"
      },
      "source": [
        "# Create Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:55.022894Z",
          "iopub.status.busy": "2025-11-30T13:55:55.022606Z",
          "iopub.status.idle": "2025-11-30T13:55:55.026936Z",
          "shell.execute_reply": "2025-11-30T13:55:55.026285Z",
          "shell.execute_reply.started": "2025-11-30T13:55:55.022875Z"
        },
        "id": "rD8taovaHqGi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import json\n",
        "import math\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:55.307432Z",
          "iopub.status.busy": "2025-11-30T13:55:55.307163Z",
          "iopub.status.idle": "2025-11-30T13:55:55.313445Z",
          "shell.execute_reply": "2025-11-30T13:55:55.312666Z",
          "shell.execute_reply.started": "2025-11-30T13:55:55.307401Z"
        },
        "id": "WFdUmc_YHqGi",
        "outputId": "179a372f-f2ca-42d2-a29e-7c5572bddf58",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Login info set!\n"
          ]
        }
      ],
      "source": [
        "import os, json\n",
        "\n",
        "my_username = \"ziad23samer\"\n",
        "my_key = \"0dda25fbbd0112eac2ff1848742972a2\"\n",
        "\n",
        "creds = {\"username\": my_username, \"key\": my_key}\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(creds, f)\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "print(\"Login info set!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:55:55.825811Z",
          "iopub.status.busy": "2025-11-30T13:55:55.825187Z",
          "iopub.status.idle": "2025-11-30T13:55:55.834408Z",
          "shell.execute_reply": "2025-11-30T13:55:55.833727Z",
          "shell.execute_reply.started": "2025-11-30T13:55:55.825785Z"
        },
        "id": "58gUugdVHqGi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "WORKING_FILE = \"/kaggle/working/arabic_diacrirization_features.h5\"\n",
        "TEMP_DIR = \"/kaggle/tmp/rescue\"\n",
        "DATASET_SLUG = f\"{api.get_config_value('username')}/arabic-diacrirization-features\"\n",
        "DATASET_TITLE = \"Rescue Data Output\"\n",
        "\n",
        "def upload_checkpoint(current_index, total_len):\n",
        "    \"\"\"\n",
        "    Copies current file to tmp and pushes to Kaggle Dataset.\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(WORKING_FILE)\n",
        "    dest_path = os.path.join(TEMP_DIR, filename)\n",
        "\n",
        "    print(f\"\\n[Checkpoint] Processing upload at index {current_index}...\")\n",
        "\n",
        "    if os.path.exists(TEMP_DIR):\n",
        "        shutil.rmtree(TEMP_DIR)\n",
        "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "\n",
        "    shutil.copy(WORKING_FILE, dest_path)\n",
        "\n",
        "    meta = {\n",
        "        \"title\": DATASET_TITLE,\n",
        "        \"id\": f\"{DATASET_SLUG}-{current_index}\",\n",
        "        \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
        "    }\n",
        "    with open(os.path.join(TEMP_DIR, 'dataset-metadata.json'), 'w') as f:\n",
        "        json.dump(meta, f)\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(f\"Creating NEW dataset: {DATASET_SLUG}-{current_index}\")\n",
        "        api.dataset_create_new(\n",
        "            folder=TEMP_DIR,\n",
        "            dir_mode='zip',\n",
        "            quiet=True\n",
        "        )\n",
        "        print(\"Upload command sent successfully.\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Upload FAILED: {e}\")\n",
        "        pass\n",
        "\n",
        "    shutil.rmtree(TEMP_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "execution": {
          "iopub.execute_input": "2025-11-30T13:56:05.444267Z",
          "iopub.status.busy": "2025-11-30T13:56:05.444001Z",
          "iopub.status.idle": "2025-11-30T14:14:27.020664Z",
          "shell.execute_reply": "2025-11-30T14:14:27.019792Z",
          "shell.execute_reply.started": "2025-11-30T13:56:05.444246Z"
        },
        "id": "zhSWIjuBN4MI",
        "outputId": "0d12a845-eaf4-4b0c-8fee-58aaa2f59342",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|â–ˆâ–‰        | 15998/80254 [03:28<11:34, 92.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Trigger] Reached 160 saves. Uploading and clearing disk...\n",
            "\n",
            "[Checkpoint] Processing upload at index 16000...\n",
            "Creating NEW dataset: ziad23samer/arabic-diacrirization-features-16000\n",
            "Upload command sent successfully.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|â–ˆâ–‰        | 16017/80254 [03:37<3:27:31,  5.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted local file zizo_part_16000.h5 to free space.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 31993/80254 [07:06<09:10, 87.69it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Trigger] Reached 160 saves. Uploading and clearing disk...\n",
            "\n",
            "[Checkpoint] Processing upload at index 32000...\n",
            "Creating NEW dataset: ziad23samer/arabic-diacrirization-features-32000\n",
            "Upload command sent successfully.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 32013/80254 [07:16<2:53:52,  4.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted local file zizo_part_32000.h5 to free space.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 47996/80254 [10:45<06:25, 83.63it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Trigger] Reached 160 saves. Uploading and clearing disk...\n",
            "\n",
            "[Checkpoint] Processing upload at index 48000...\n",
            "Creating NEW dataset: ziad23samer/arabic-diacrirization-features-48000\n",
            "Upload command sent successfully.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48012/80254 [10:55<2:19:00,  3.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted local file zizo_part_48000.h5 to free space.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 63998/80254 [14:25<02:54, 93.29it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Trigger] Reached 160 saves. Uploading and clearing disk...\n",
            "\n",
            "[Checkpoint] Processing upload at index 64000...\n",
            "Creating NEW dataset: ziad23samer/arabic-diacrirization-features-64000\n",
            "Upload command sent successfully.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 64016/80254 [14:35<58:42,  4.61it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted local file zizo_part_64000.h5 to free space.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 79993/80254 [18:05<00:02, 88.50it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Trigger] Reached 160 saves. Uploading and clearing disk...\n",
            "\n",
            "[Checkpoint] Processing upload at index 80000...\n",
            "Creating NEW dataset: ziad23samer/arabic-diacrirization-features-80000\n",
            "Upload command sent successfully.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 80011/80254 [18:15<00:53,  4.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted local file zizo_part_80000.h5 to free space.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 80253/80254 [18:18<00:00, 80.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Trigger] Reached 3 saves. Uploading and clearing disk...\n",
            "\n",
            "[Checkpoint] Processing upload at index 80254...\n",
            "Creating NEW dataset: ziad23samer/arabic-diacrirization-features-80254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80254/80254 [18:21<00:00, 72.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload command sent successfully.\n",
            "\n",
            "Deleted local file zizo_part_80254.h5 to free space.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "H5_PATH = \"/kaggle/working/arabic_diacrirization_features.h5\"\n",
        "CHECKPOINT_INDEX_PATH = \"/kaggle/working/arabic_diacrirization_features.txt\"\n",
        "\n",
        "SAVE_EVERY = 100\n",
        "UPLOAD_EVERY_N_SAVES = 160\n",
        "\n",
        "def map_tashkeel(char_list):\n",
        "    return [diacritic2id[c] for c in char_list]\n",
        "\n",
        "start_index = 0\n",
        "if os.path.exists(CHECKPOINT_INDEX_PATH):\n",
        "    with open(CHECKPOINT_INDEX_PATH, \"r\") as f:\n",
        "        start_index = int(f.read())\n",
        "    print(f\"Resuming from index: {start_index}\")\n",
        "\n",
        "buffer_features = []\n",
        "buffer_tashkeel = []\n",
        "save_counter = 0\n",
        "\n",
        "if start_index == 0 and os.path.exists(H5_PATH):\n",
        "    os.remove(H5_PATH)\n",
        "\n",
        "for i in tqdm(range(start_index, len(new_sentences))):\n",
        "    sent = new_sentences[i]\n",
        "\n",
        "    f, t = zizo_features(sent, i, bert_model, bert_tokenizer)\n",
        "\n",
        "    buffer_features.extend(f)\n",
        "    t_ids = map_tashkeel(t)\n",
        "    buffer_tashkeel.extend(t_ids)\n",
        "\n",
        "    assert len(f) == len(t_ids), \"They should be equal\"\n",
        "\n",
        "    if (i + 1) % SAVE_EVERY == 0 or i == len(new_sentences) - 1:\n",
        "\n",
        "        batch_features = np.array(buffer_features, dtype=np.float16)\n",
        "        batch_tashkeel = np.array(buffer_tashkeel, dtype=np.int32)\n",
        "\n",
        "        with h5py.File(H5_PATH, 'a') as hf:\n",
        "            if 'features' not in hf:\n",
        "                hf.create_dataset('features', data=batch_features,\n",
        "                                  maxshape=(None, 1024),\n",
        "                                  chunks=True,\n",
        "                                  compression=\"lzf\")\n",
        "                hf.create_dataset('tashkeel', data=batch_tashkeel,\n",
        "                                  maxshape=(None,),\n",
        "                                  chunks=True,\n",
        "                                  compression=\"lzf\")\n",
        "            else:\n",
        "                hf['features'].resize((hf['features'].shape[0] + batch_features.shape[0]), axis=0)\n",
        "                hf['tashkeel'].resize((hf['tashkeel'].shape[0] + batch_tashkeel.shape[0]), axis=0)\n",
        "\n",
        "                hf['features'][-batch_features.shape[0]:] = batch_features\n",
        "                hf['tashkeel'][-batch_tashkeel.shape[0]:] = batch_tashkeel\n",
        "\n",
        "        with open(CHECKPOINT_INDEX_PATH, \"w\") as f:\n",
        "            f.write(str(i + 1))\n",
        "\n",
        "        buffer_features = []\n",
        "        buffer_tashkeel = []\n",
        "\n",
        "        save_counter += 1\n",
        "\n",
        "        if save_counter >= UPLOAD_EVERY_N_SAVES or (i == len(new_sentences) - 1 and save_counter > 0):\n",
        "            print(f\"\\n[Trigger] Reached {save_counter} saves. Uploading and clearing disk...\")\n",
        "\n",
        "            part_filename = f\"zizo_part_{i+1}.h5\"\n",
        "            part_path = f\"/kaggle/working/{part_filename}\"\n",
        "\n",
        "            shutil.move(H5_PATH, part_path)\n",
        "\n",
        "            global WORKING_FILE\n",
        "            WORKING_FILE = part_path\n",
        "\n",
        "            try:\n",
        "                upload_checkpoint(i + 1, len(new_sentences))\n",
        "                if os.path.exists(part_path):\n",
        "                    os.remove(part_path)\n",
        "                    print(f\"Deleted local file {part_filename} to free space.\")\n",
        "\n",
        "                save_counter = 0\n",
        "            except Exception as e:\n",
        "                print(f\"Upload failed: {e}\")\n",
        "                shutil.move(part_path, H5_PATH)\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uvt4-ioKOsB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# import os\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# DRIVE_CHECKPOINT_PATH = \"/kaggle/working/zizo_checkpoint.pkl\"\n",
        "\n",
        "# final_features = []\n",
        "# final_tashkeel = []\n",
        "\n",
        "# START_INDEX = 0\n",
        "# SAVE_EVERY = 100\n",
        "\n",
        "# # Resume if Drive checkpoint exists\n",
        "# if os.path.exists(DRIVE_CHECKPOINT_PATH):\n",
        "#     print(\"ğŸ” Resuming from checkpoint...\")\n",
        "#     with open(DRIVE_CHECKPOINT_PATH, \"rb\") as f:\n",
        "#         data = pickle.load(f)\n",
        "#         final_features = data[\"features\"]\n",
        "#         final_tashkeel = data[\"tashkeel\"]\n",
        "#         START_INDEX = data[\"index\"] + 1\n",
        "#     print(f\"â¡ï¸ Resumed from index: {START_INDEX}\")\n",
        "\n",
        "# for i in tqdm(range(START_INDEX, len(sentences))):\n",
        "#     sent = sentences[i]\n",
        "\n",
        "#     f, t = zizo_features(sent, i, bert_model, bert_tokenizer, device)\n",
        "#     final_features.append(f)\n",
        "#     final_tashkeel.append(t)\n",
        "\n",
        "#     if (i + 1) % SAVE_EVERY == 0 or i == len(sentences) - 1:\n",
        "#         checkpoint_data = {\n",
        "#             \"features\": final_features,\n",
        "#             \"tashkeel\": final_tashkeel,\n",
        "#             \"index\": i\n",
        "#         }\n",
        "#         with open(DRIVE_CHECKPOINT_PATH, \"wb\") as f:\n",
        "#             pickle.dump(checkpoint_data, f)\n",
        "\n",
        "#         print(f\"ğŸ’¾ Direct checkpoint saved to Drive at index {i}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRBi18nrxv2t",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def extract_all_features(\n",
        "#     sentence: str,\n",
        "#     sent_index: int,\n",
        "#     arabert_model=None,\n",
        "#     arabert_tokenizer=None,\n",
        "#     araelectra_model=None,\n",
        "#     araelectra_tokenizer=None,\n",
        "#     fasttext_model=None,\n",
        "#     flair_char_embed=None,\n",
        "#     buckwalter_enabled=True,\n",
        "#     device = \"cpu\"\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Extract all features for a single sentence.\n",
        "\n",
        "#     Returns a dictionary with:\n",
        "#     - tokens\n",
        "#     - ArabERT embeddings (list of token embeddings)\n",
        "#     - POS tags\n",
        "#     - Flair char embeddings (list of char-level embeddings)\n",
        "#     - Tashkeel sequence (diacritics)\n",
        "#     - Buckwalter transliteration (optional)\n",
        "#     - FastText embeddings (list of word embeddings)\n",
        "#     - AraELECTRA sentence embedding\n",
        "#     \"\"\"\n",
        "\n",
        "#     features = {}\n",
        "\n",
        "#     # --- Tokens + ArabERT embeddings ---\n",
        "#     arabert_emb, tokens = get_arabert_embeddings(sentence)\n",
        "#     features[\"tokens\"] = tokens\n",
        "#     features[\"arabert_embeddings\"] = arabert_emb\n",
        "\n",
        "#     # --- POS tags ---\n",
        "#     features[\"pos\"] = extract_pos_tags(sentence)\n",
        "\n",
        "#     # --- Flair char embeddings ---\n",
        "#     features[\"char_embeddings\"] = extract_char_embeddings(sentence) if flair_char_embed else None\n",
        "\n",
        "#     # --- Tashkeel / diacritics ---\n",
        "#     features[\"diacritics\"] = get_tashkeel_sequence(sent_index)\n",
        "\n",
        "#     # --- Buckwalter transliteration ---\n",
        "#     if buckwalter_enabled:\n",
        "#         from lang_trans.arabic import buckwalter\n",
        "#         features[\"buckwalter\"] = buckwalter.transliterate(sentence)\n",
        "#         features['tf-idf']=tfidf_df.loc[sent_index]\n",
        "#         features['bow']=bow_df.loc[sent_index]\n",
        "\n",
        "#     # --- FastText embeddings ---\n",
        "#     if fasttext_model:\n",
        "#         # split sentence into words\n",
        "#         words = sentence.split()\n",
        "#         features[\"fasttext_embeddings\"] = [fasttext_model.wv[word] if word in fasttext_model.wv else None for word in words]\n",
        "\n",
        "#     # --- AraELECTRA sentence embedding ---\n",
        "#     if araelectra_model and araelectra_tokenizer:\n",
        "#         sent_emb = get_araelectra_embeddings(sentence, araelectra_model, araelectra_tokenizer)\n",
        "#         features[\"araelectra_embedding\"] = sent_emb\n",
        "\n",
        "#     return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PYgcqRCxytd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "features = extract_all_features(sentences[0], 0,bert_model,bert_tokenizer,electra_model,electra_tokenizer,\n",
        "    ft_model,True,True,device)\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9h8wHUl9Rsb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(features.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft5WtalWyLrC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def extract_features_for_all(sentences):\n",
        "    all_features = []\n",
        "    for i, sent in tqdm(enumerate(sentences), total=len(sentences)):\n",
        "        feats = extract_all_features(sent, i,bert_model,bert_tokenizer,electra_model,electra_tokenizer,\n",
        "    ft_model,\n",
        "    True,\n",
        "    True,device)\n",
        "        all_features.append(feats)\n",
        "    return all_features\n",
        "\n",
        "\n",
        "# ---- Run feature extraction for the whole dataset ----\n",
        "full_feature_dataset = extract_features_for_all(sentences)\n",
        "\n",
        "# ---- Save to file ----\n",
        "with open(\"arabic_diacritization_features.pkl\", \"wb\") as f:\n",
        "    pickle.dump(full_feature_dataset, f)\n",
        "\n",
        "print(\"ğŸ¯ All features extracted and saved successfully!\")\n",
        "print(\"ğŸ“ Output file: arabic_diacritization_features.pkl\")\n",
        "print(\"Total samples:\", len(full_feature_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thkGy2YYxV7A"
      },
      "source": [
        "# **AraVec** (still not working)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jytcY5ntSjbG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !unzip full_grams_cbow_300_twitter.zip -d aravec_twitter_cbow_300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f7AApU2ckJr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from gensim.models import KeyedVectors\n",
        "\n",
        "# model_path = \"full_grams_cbow_300_twitter.mdl\"\n",
        "\n",
        "# aravec = KeyedVectors.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQYKP81Uco42",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def tokenize(text):\n",
        "#     text = arabert_prep.preprocess(text)\n",
        "#     return text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08NW6_F6HqGm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import shutil, os, json\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Keep this exactly as you had it\n",
        "source_path = \"/kaggle/working/zizo_dataset_2.h5\"\n",
        "# ---------------------\n",
        "\n",
        "# 1. Move file to /kaggle/tmp\n",
        "# We extract just the name \"zizo_dataset_2.h5\" to avoid the path error\n",
        "filename_only = os.path.basename(source_path)\n",
        "dest_path = os.path.join(\"/kaggle/tmp/rescue\", filename_only)\n",
        "\n",
        "print(f\"Moving {filename_only} to temporary storage...\")\n",
        "os.makedirs(\"/kaggle/tmp/rescue\", exist_ok=True)\n",
        "shutil.move(source_path, dest_path)\n",
        "\n",
        "# 2. Create Dataset Metadata\n",
        "with open('/root/.kaggle/kaggle.json') as f:\n",
        "    creds = json.load(f)\n",
        "\n",
        "meta = {\n",
        "    \"title\": \"Rescue Data Output\",\n",
        "    \"id\": f\"{creds['username']}/rescue-data-output\",\n",
        "    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
        "}\n",
        "with open('/kaggle/tmp/rescue/dataset-metadata.json', 'w') as f:\n",
        "    json.dump(meta, f)\n",
        "\n",
        "# 3. Upload to Kaggle\n",
        "print(\"Starting upload... This will take a few minutes.\")\n",
        "!kaggle datasets create -p /kaggle/tmp/rescue --dir-mode zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:42:55.428331Z",
          "iopub.status.busy": "2025-11-30T13:42:55.427732Z",
          "iopub.status.idle": "2025-11-30T13:42:55.432875Z",
          "shell.execute_reply": "2025-11-30T13:42:55.432214Z",
          "shell.execute_reply.started": "2025-11-30T13:42:55.4283Z"
        },
        "id": "7ZMowVbSHqGm",
        "outputId": "7f5acd37-6d7d-482b-c3eb-fc8d4254f85e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "80254"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(new_sentences)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "JBjJG5DFHqGm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-30T13:54:14.284239Z",
          "iopub.status.busy": "2025-11-30T13:54:14.283972Z",
          "iopub.status.idle": "2025-11-30T13:54:14.289441Z",
          "shell.execute_reply": "2025-11-30T13:54:14.288728Z",
          "shell.execute_reply.started": "2025-11-30T13:54:14.284219Z"
        },
        "id": "H-q8MK81HqGm",
        "outputId": "3671960f-19e2-43ca-a83d-80e8fe6c7940",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<PAD>': 0,\n",
              " 'Ùˆ': 2,\n",
              " 'Ù„': 3,\n",
              " ' ': 4,\n",
              " 'Ø¬': 5,\n",
              " 'Ù…': 6,\n",
              " 'Ø¹': 7,\n",
              " 'Ø«': 8,\n",
              " 'Øª': 9,\n",
              " 'Ø±': 10,\n",
              " 'Ùƒ': 11,\n",
              " 'Ù†': 12,\n",
              " 'Ø§': 13,\n",
              " 'Ø£': 14,\n",
              " 'Ù‰': 15,\n",
              " 'Ø¨': 16,\n",
              " 'Ø·': 17,\n",
              " 'ÙŠ': 18,\n",
              " 'Ø¯': 19,\n",
              " 'Ù‡': 20,\n",
              " 'ØŒ': 21,\n",
              " 'Ø©': 22,\n",
              " 'Ù': 23,\n",
              " 'Ø¥': 24,\n",
              " 'Ù‚': 25,\n",
              " 'Ø²': 26,\n",
              " 'Ø¤': 27,\n",
              " 'Ø¶': 28,\n",
              " 'Ø°': 29,\n",
              " 'Ø³': 30,\n",
              " 'Ø®': 31,\n",
              " 'Ø­': 32,\n",
              " 'Ø¸': 33,\n",
              " 'Ø¡': 34,\n",
              " '.': 35,\n",
              " ':': 36,\n",
              " '{': 37,\n",
              " '}': 38,\n",
              " 'Ø´': 39,\n",
              " '[': 40,\n",
              " ']': 41,\n",
              " 'Øµ': 42,\n",
              " '(': 43,\n",
              " 'Øº': 44,\n",
              " ')': 45,\n",
              " 'Ø›': 46,\n",
              " 'Ø¦': 47,\n",
              " 'ØŸ': 48,\n",
              " 'Ø¢': 49,\n",
              " '-': 50,\n",
              " '!': 51,\n",
              " 'Â«': 52,\n",
              " 'Â»': 53,\n",
              " '?': 1}"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnIPDSP6HqGm",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "FeatureExtraction",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8855126,
          "sourceId": 13898942,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8876099,
          "sourceId": 13928805,
          "sourceType": "datasetVersion"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 519934,
          "modelInstanceId": 505006,
          "sourceId": 667079,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
